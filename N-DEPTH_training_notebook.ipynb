{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.008062,
     "end_time": "2024-10-07T10:47:45.872605",
     "exception": false,
     "start_time": "2024-10-07T10:47:45.864543",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Training Script\n",
    "### [N-DEPTH: Neural Depth Encoding for Compression-Resilient 3D Video Streaming](https://www.mdpi.com/2079-9292/13/13/2557)\n",
    "### by [Stephen Siemonsma](https://github.com/ssiemonsma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005946,
     "end_time": "2024-10-07T10:47:45.885103",
     "exception": false,
     "start_time": "2024-10-07T10:47:45.879157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 1.597156,
     "end_time": "2024-10-07T10:47:47.487579",
     "exception": false,
     "start_time": "2024-10-07T10:47:45.890423",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from abc import ABC\n",
    "from enum import Enum\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm  # Compatible with Papermill (for running notebook in command line), but may result in very large .ipynb files if you print updates too often\n",
    "# from tqdm.notebook import tqdm  # Note that using tqdm.notebook (as opposed to the standard tqdm library) will make the print logs less persistent if you ever move the notebook file.  The regular tqdm library will make the printed logs be more persistent, but this can result in ridiculously large notebook files that will load very slowly if you aren't careful.\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Torchvision imports\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import _read_pfm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# IPython-specific imports\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009083,
     "end_time": "2024-10-07T10:47:47.503513",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.494430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005441,
     "end_time": "2024-10-07T10:47:47.514595",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.509154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Jupyter View Tweaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.010779,
     "end_time": "2024-10-07T10:47:47.530780",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.520001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Widens cells to fill window\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005423,
     "end_time": "2024-10-07T10:47:47.541729",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.536306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Performance Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.010218,
     "end_time": "2024-10-07T10:47:47.557363",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.547145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit PyTorch to 1 thread.\n",
    "# This is a guard against a PyTorch bug where the CPU can sometimes be bogged down with nearly 100% utilization, despite doing little work (the data loader is not nearly that intensive).\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005468,
     "end_time": "2024-10-07T10:47:47.568319",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.562851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configuration Enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009477,
     "end_time": "2024-10-07T10:47:47.583261",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.573784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrecisionMode(Enum):\n",
    "    MIXED_PRECISION = 1\n",
    "    PYTORCH_DEFAULT = 2\n",
    "    FLOAT32_PRECISION = 3\n",
    "\n",
    "class QuantizerProxyType(Enum):\n",
    "    STRAIGHT_THROUGH = \"straight-through\"  # Generally trains faster\n",
    "    SOFT = \"soft\"\n",
    "\n",
    "class ImageCompressionType(Enum):\n",
    "    DIFF_JPEG = \"DiffJPEG\"\n",
    "    JPEG = \"JPEG\"\n",
    "    LOSSLESS = \"lossless\"\n",
    "\n",
    "class MaskType(Enum):\n",
    "    BINARY = \"binary\"\n",
    "    ERROR_THRESHOLDED_BINARY = \"error-thresholded binary\"  # This is what I had the best luck with.  The binary mask logits can be be treated as a \"confidence map\" in practice.\n",
    "    ERROR_MAP = \"error map\"\n",
    "    \n",
    "class FinalEncoderActivationFunction(Enum):\n",
    "    SINE = \"sine\"\n",
    "    SIGMOID = \"sigmoid\"\n",
    "    NONE = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005602,
     "end_time": "2024-10-07T10:47:47.594483",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.588881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Function to change parameters relevant to PyTorch's numerical precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009742,
     "end_time": "2024-10-07T10:47:47.609771",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.600029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_performance_and_precision_settings(precision_mode):\n",
    "    match precision_mode:\n",
    "        case PrecisionMode.MIXED_PRECISION:\n",
    "            # See: https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere\n",
    "            torch.backends.cuda.matmul.allow_tf32 = False  # False by default.  This flag controls whether to allow TF32 on matmul.\n",
    "            torch.backends.cudnn.allow_tf32 = True  # True by default.  This flag controls whether to allow TF32 on cuDNN (for convolutions).\n",
    "            scaler = torch.amp.GradScaler('cuda')\n",
    "        case PrecisionMode.PYTORCH_DEFAULT:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = False  # False by default.  This flag controls whether to allow TF32 on matmul.\n",
    "            torch.backends.cudnn.allow_tf32 = True  # True by default.  This flag controls whether to allow TF32 on cuDNN (for convolutions).\n",
    "            scaler = None\n",
    "        case PrecisionMode.FLOAT32_PRECISION:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = False  # False by default.  This flag controls whether to allow TF32 on matmul.\n",
    "            torch.backends.cudnn.allow_tf32 = False  # True by default.  This flag controls whether to allow TF32 on cuDNN (for convolutions).\\\n",
    "            scaler = None\n",
    "        case PrecisionMode.FLOAT64_PRECISION:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = False  # False by default.  This flag controls whether to allow TF32 on matmul.\n",
    "            torch.backends.cudnn.allow_tf32 = False  # True by default.  This flag controls whether to allow TF32 on cuDNN (for convolutions).\n",
    "            scaler = None\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True  # Allows cuDNN to dynamically select the most efficient convolution algorithms based on runtime benchmarks\n",
    "    torch.backends.cudnn.deterministic = False  # Set to True for deterministic behavior (helpful when reproducibility is key)\n",
    "\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005506,
     "end_time": "2024-10-07T10:47:47.620869",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.615363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Papermill Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.008371,
     "end_time": "2024-10-07T10:47:47.654205",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.645834",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# If running without Jupyter Notebook or Jupyter Lab, Papermill can be configured to provide the file path of the notebook it is running (so that you can use this file name for naming checkpoint files, etc.).\n",
    "# Papermill is great for running the notebook entirely from the command line while retaining the ability to save all Jupyter cell outputs.\n",
    "\n",
    "# Defaults\n",
    "PAPERMILL_INPUT_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd02cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.008328,
     "end_time": "2024-10-07T10:47:47.668146",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.659818",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "PAPERMILL_INPUT_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005603,
     "end_time": "2024-10-07T10:47:47.679347",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.673744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Parsing Notebook Name and Logging Run Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.010013,
     "end_time": "2024-10-07T10:47:47.694975",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.684962",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parsing the notebook name\n",
    "if PAPERMILL_INPUT_PATH is not None:\n",
    "    notebook_path = PAPERMILL_INPUT_PATH\n",
    "else:\n",
    "    # If ran interactively, we can get the notbook path this way\n",
    "    notebook_path = os.getenv('JPY_SESSION_NAME')\n",
    "notebook_name = os.path.splitext(os.path.basename(notebook_path))[0]\n",
    "print(\"Notebook name:\", notebook_name)\n",
    "print(\"Date Ran:\", datetime.now().strftime(\"%d/%m/%Y %I:%M:%S %p\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005514,
     "end_time": "2024-10-07T10:47:47.705943",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.700429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Config Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.008381,
     "end_time": "2024-10-07T10:47:47.719825",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.711444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Empty class to make adding new configuration variables very trivial.  Treated similarly to a dictionary, but with more convenient dot notation.\n",
    "# The config object will be saved with Pickle alongside the model checkpoints.\n",
    "class Config:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005746,
     "end_time": "2024-10-07T10:47:47.731128",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.725382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.050678,
     "end_time": "2024-10-07T10:47:47.787582",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.736904",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Debugging\n",
    "config.dry_run = False  # If enabled, model checkpointing and other logging will not be enabled (for debugging and evaluating performance)\n",
    "config.debug_mode = False  # This will limit the size of each \"run\" so that the script can be tested end-to-end.  This will write out checkpoints, etc. to validate those features.\n",
    "config.debug_mode_loop_length = 3\n",
    "\n",
    "# Important\n",
    "config.start_from_scratch = True  # If True, all model checkpoints matching derived from the notebook name will be deleted.  If False, training will continue from a checkpoint file.\n",
    "config.weight_initialization_source = None  # For loading a pretrained model from a different folder.  Not necessary if simply continuing training from a checkpoint in the folder expected from the notebook name.\n",
    "# config.weight_initialization_source = \"weights/N-DEPTH_training_run3_best_L1_rangeGradient_val_loss.pt\"\n",
    "config.ignore_mismatched_weight_sizes = True  # Just in case you are loading some weights from a a model where other model components have been changed (e.g., loading pre-trained encoder weights to help train a new decoder structure)\n",
    "config.dataset_path = \"/path_to_parent_directory_of_FlyingThings3D\"  # Set this to the path of the parent directory of the FlyingThings3D dataset\n",
    "\n",
    "# Precision\n",
    "config.precision_mode = PrecisionMode.MIXED_PRECISION\n",
    "# config.precision_mode = PrecisionMode.PYTORCH_DEFAULT\n",
    "# config.precision_mode = PrecisionMode.FLOAT32_PRECISION\n",
    "scaler = set_performance_and_precision_settings(config.precision_mode)\n",
    "\n",
    "# Quantization\n",
    "config.differentiable_rounding_type = QuantizerProxyType.STRAIGHT_THROUGH\n",
    "# config.differentiable_rounding_type = QuantizerProxyType.SOFT\n",
    "\n",
    "# Encoder\n",
    "config.encoder_activation_function = nn.Mish(inplace=True)\n",
    "# config.encoder_activation_function = nn.ReLU(inplace=True)\n",
    "# config.encoder_activation_function = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "config.final_encoder_activation_function = FinalEncoderActivationFunction.SINE\n",
    "# config.final_encoder_activation_function = FinalEncoderActivationFunction.SIGMOID\n",
    "# config.final_encoder_activation_function = FinalEncoderActivationFunction.NONE  # Probably should only use with MWD encoder, otherwise value range will be invalid for DiffJPEG\n",
    "config.neural_encoder_width = 2048  # Number of neurons in the largest 1st layer of the encoder MLP\n",
    "\n",
    "# Decoder\n",
    "config.decoder_activation_function = nn.Mish(inplace=True)  # Generally converged best\n",
    "# config.decoder_activation_function = nn.ReLU(inplace=True)\n",
    "# config.decoder_activation_function = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "config.neural_decoder_width = 2048  # Number of neurons in the largest initial layer of the decoder MLP (other layer sizes scaled from this)\n",
    "\n",
    "# Weight Freezing\n",
    "config.freeze_encoder = False\n",
    "config.freeze_decoder = False\n",
    "\n",
    "# Misc\n",
    "config.expanded_training_range = 1.02  # \"Overprovisioning\" the depth range ensures that you can avoid the high-error regions at the beginning and end of the range\n",
    "\n",
    "# Training Parameters\n",
    "config.num_training_epochs = 300  # Maximum total number of training epochs among all training runs\n",
    "config.batch_size = 1  # Note: Batches greater than 1 will likely need to edit a bit of code.  1 image is effectively thousands of samples for this network, so larger batch sizes are less necessary.\n",
    "config.height = 224  # DiffJPEG-compatible resolutions are, at a minimum multiples of 8 (due to 8x8 image blocks)\n",
    "config.width = 224\n",
    "config.random_crops = True\n",
    "config.depth_normalization_lower_bound = 0  # I didn't notice any benefits to other normalization ranges (e.g., [-1, 1])\n",
    "config.depth_normalization_upper_bound = 1\n",
    "config.device = torch.device(\"cuda\") # For NVIDIA cards\n",
    "# config.device = torch.device(\"mps\") # For M-series Macs\n",
    "# config.device = torch.device(\"cpu\") # For slow training on CPU\n",
    "\n",
    "# Image Settings\n",
    "config.image_compression_type = ImageCompressionType.DIFF_JPEG\n",
    "# config.image_compression_type = ImageCompressionType.LOSSLESS  # Can train without JPEG compression\n",
    "config.low_JPEG_quality = 85  # Note: This JPEG quality training range is somewhat arbitrary.\n",
    "config.high_JPEG_quality = 99.999\n",
    "config.subsampling = True  # 4:2:0 chroma subsampling, if enabled, otherwise 4:4:4 chroma sampling.  Generally 4:2:0 chroma subsampling will result in encoding functions that are more visually pleasing when plotte in 3D.\n",
    "config.color_space_conversions = True  # True by default for JPEG, but you can disable this to save directly to YCbCr.  Note: Chroma subsampling is not yet an option when saving directly into YCbCr without color space conversions.\n",
    "config.gradient_validation_image_compression_type = ImageCompressionType.LOSSLESS  # Note: Lossy compression not yet supported for 100x100 gradient image due to DiffJPEG restrictions on image resolutions.  The easiest workaround would be using a compatible gradient image resolution.\n",
    "config.target_validation_JPEG_quality = 99.999\n",
    "config.interpolation_mode = T.functional.InterpolationMode.NEAREST_EXACT  # Important!: Using anything except nearest neighbor interpolation will result in \"fake\" interpolated depth values, which will greatly affect JPEG artifacting at edges.\n",
    "\n",
    "# Loss Parameters\n",
    "config.depth_loss_enabled = True\n",
    "config.mask_loss_enabled = True\n",
    "config.depth_gradient_val_interval = 10  # This is then number of mini-batches between the lossless evaluation of a \"tilted plane\" with 100x100 resolution\n",
    "config.JPEG_normalization_interval = 10  # This is then number of mini-batches between each calibration step that determines how to normalize losses given a particular JPEG quality setting\n",
    "config.depth_loss_weighting = 6\n",
    "config.mask_loss_weighting = 0.06  # Set to a low value to prioritize depth reconstruction quality\n",
    "config.depth_loss_function = nn.L1Loss()  # Encourages better reconstructions of interior regions (not as distracted by JPEG artifacts along edges)\n",
    "# config.depth_loss_function = nn.MSELoss()  # Will likely decrease JPEG artifacting, but at the cost of interior depth reconstruction quality.\n",
    "# config.mask_type = MaskType.BINARY\n",
    "config.mask_type = MaskType.ERROR_THRESHOLDED_BINARY\n",
    "# config.mask_type = MaskType.ERROR_MAP\n",
    "config.mask_loss_function = nn.BCEWithLogitsLoss()\n",
    "# config.mask_loss_function = nn.L1Loss()  # Would be an appropriate option if attempting to generate an error map (i.e., not a binary mask)\n",
    "config.error_threshold_for_masking = 0.03  # If decoded error is >3% of normalized range, the network will be trained to filter out this type of corrupted pixel\n",
    "config.weight_decay = 0  # Regularization, if needed\n",
    "\n",
    "# Optimizer/Scheduler\n",
    "config.optimizer_type = optim.Adam\n",
    "# config.optimizer_type = optim.SGD\n",
    "config.scheduler_type = optim.lr_scheduler.CosineAnnealingWarmRestarts  # Cyclic and more aggressive (since overfitting is not a high risk)\n",
    "# config.scheduler_type = optim.lr_scheduler.ReduceLROnPlateau\n",
    "config.starting_lr = 1e-4\n",
    "config.cosine_annealing_scheduler_period = 5000\n",
    "config.cosine_annealing_scheduler_restart_factor = 1  # Set to less than 1 if you want the peak learning rate to decrease after every cycle\n",
    "config.lr_gamma = 0.5 # (for metric-based scheduler ony) This is the factor the learning rate decreases by after the metric doesn't improve for some time\n",
    "config.patience = 50000  # (for metric-based scheduler only) The number of iterations that must pass without metric improvement for the learning rate to decrease\n",
    "\n",
    "# Training Loop Parameters\n",
    "config.stalled_val_patience = 10  # Training will end after this many epochs is the validation loss has not improved by more than the threshold amount over that time\n",
    "config.stalled_val_threshold = 1e-6\n",
    "config.max_epochs_per_run = 100  # Training run will end after this number of epochs, even if progress is still somehow being made\n",
    "config.num_training_epochs = 3 * config.max_epochs_per_run  # Just used to initialize some metric arrays\n",
    "\n",
    "# Logging\n",
    "config.encoded_image_log_interval = 500  # Measured in mini-batches\n",
    "config.tensorboard_log_interval = 500\n",
    "\n",
    "# If runninning in Papermill, the TQDM print frequency will be decreased\n",
    "if PAPERMILL_INPUT_PATH is not None:\n",
    "    config.print_interval = 500  # Lower print frequency when ran with Papermill\n",
    "else:\n",
    "    config.print_interval = 20  # Higher print frequency for interactive runs\n",
    "\n",
    "if config.debug_mode:\n",
    "    config.print_interval = 1  # Highest print frequency for debug runs (since they may be very short)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.006438,
     "end_time": "2024-10-07T10:47:47.800169",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.793731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.010677,
     "end_time": "2024-10-07T10:47:47.816457",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.805780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since the network can improve very rapidly in the first few epochs (causing the generation of a metric-based checkpoint file), caching checkpoint files in memory until the end of the epoch is significantly faster.\n",
    "def cache_checkpoint(config, epoch, net, optimizer, scheduler, scaler,\n",
    "                    best_depth_range_gradient_L1_loss, best_max_depth_range_gradient_L1_loss, \n",
    "                    best_avg_L1_loss_on_JPEG_normalization_image, best_avg_RMSE_loss_on_JPEG_normalization_image, best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                    best_depth_range_gradient_L1_losses_by_epoch, best_max_depth_range_gradient_L1_losses_by_epoch, best_avg_L1_losses_on_JPEG_normalization_by_epoch, \n",
    "                    best_avg_RMSE_losses_on_JPEG_normalization_by_epoch, best_avg_mask_loss_on_JPEG_normalization_image_by_epoch):\n",
    "    global global_step\n",
    "\n",
    "    if config.precision_mode == PrecisionMode.MIXED_PRECISION:\n",
    "        cached_checkpoint = {\n",
    "                                'epoch': epoch,\n",
    "                                'global_step': global_step,\n",
    "                                'model_state_dict': net.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                                'scaler_state_dict': scaler.state_dict(),\n",
    "                                'best_depth_range_gradient_L1_loss': best_depth_range_gradient_L1_loss,\n",
    "                                'best_max_depth_range_gradient_L1_loss': best_max_depth_range_gradient_L1_loss,\n",
    "                                'best_avg_L1_loss_on_JPEG_normalization_image': best_avg_L1_loss_on_JPEG_normalization_image,\n",
    "                                'best_avg_RMSE_loss_on_JPEG_normalization_image': best_avg_RMSE_loss_on_JPEG_normalization_image,\n",
    "                                'best_avg_mask_loss_on_JPEG_normalization_image': best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                                'best_depth_range_gradient_L1_losses_by_epoch': best_depth_range_gradient_L1_losses_by_epoch,\n",
    "                                'best_max_depth_range_gradient_L1_losses_by_epoch': best_max_depth_range_gradient_L1_losses_by_epoch,\n",
    "                                'best_avg_L1_losses_on_JPEG_normalization_by_epoch': best_avg_L1_losses_on_JPEG_normalization_by_epoch,\n",
    "                                'best_avg_RMSE_losses_on_JPEG_normalization_by_epoch': best_avg_RMSE_losses_on_JPEG_normalization_by_epoch,\n",
    "                                'best_avg_mask_loss_on_JPEG_normalization_image_by_epoch': best_avg_mask_loss_on_JPEG_normalization_image_by_epoch,\n",
    "                            }\n",
    "    else:\n",
    "        cached_checkpoint = {\n",
    "                                'epoch': epoch,\n",
    "                                'global_step': global_step,\n",
    "                                'model_state_dict': net.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                                'best_depth_range_gradient_L1_loss': best_depth_range_gradient_L1_loss,\n",
    "                                'best_max_depth_range_gradient_L1_loss': best_max_depth_range_gradient_L1_loss,\n",
    "                                'best_avg_L1_loss_on_JPEG_normalization_image': best_avg_L1_loss_on_JPEG_normalization_image,\n",
    "                                'best_avg_RMSE_loss_on_JPEG_normalization_image': best_avg_RMSE_loss_on_JPEG_normalization_image,\n",
    "                                'best_avg_mask_loss_on_JPEG_normalization_image': best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                                'best_depth_range_gradient_L1_losses_by_epoch': best_depth_range_gradient_L1_losses_by_epoch,\n",
    "                                'best_max_depth_range_gradient_L1_losses_by_epoch': best_max_depth_range_gradient_L1_losses_by_epoch,\n",
    "                                'best_avg_L1_losses_on_JPEG_normalization_by_epoch': best_avg_L1_losses_on_JPEG_normalization_by_epoch,\n",
    "                                'best_avg_RMSE_losses_on_JPEG_normalization_by_epoch': best_avg_RMSE_losses_on_JPEG_normalization_by_epoch,\n",
    "                                'best_avg_mask_loss_on_JPEG_normalization_image_by_epoch': best_avg_mask_loss_on_JPEG_normalization_image_by_epoch,\n",
    "                            }\n",
    "\n",
    "    cached_checkpoint = copy.deepcopy(cached_checkpoint)\n",
    "\n",
    "    return cached_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009495,
     "end_time": "2024-10-07T10:47:47.831715",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.822220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saves a cached checkpoint to disk (with some retry attempts if this fails for some reason)\n",
    "def save_checkpoint(cached_checkpoint, checkpoint_save_name, config, weight_save_directory, notebook_name, training_run_number):\n",
    "    \n",
    "    global global_step\n",
    "    \n",
    "    if not config.dry_run:\n",
    "        max_retries = 10\n",
    "        retry_delay = 5  # seconds\n",
    "\n",
    "        full_checkpoint_save_name = weight_save_directory + notebook_name + \"_run%i_\" % training_run_number + checkpoint_save_name + \".pt\" \n",
    "            \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                torch.save(cached_checkpoint, full_checkpoint_save_name)\n",
    "            except RuntimeError as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Error occurred while saving checkpoint. Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Skipping checkpoint save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.01126,
     "end_time": "2024-10-07T10:47:47.848669",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.837409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loads a model checkpoint file (model weights, optimizer/scheduler states, and some metrics that have been saved alongside it)\n",
    "def load_checkpoint_file(checkpoint_path):\n",
    "    global global_step\n",
    "    global net\n",
    "    global optimizer\n",
    "    global scheduler\n",
    "    global scaler\n",
    "    global starting_epoch\n",
    "    global best_max_depth_range_gradient_L1_loss\n",
    "    global best_depth_range_gradient_L1_loss\n",
    "    global best_avg_L1_loss_on_JPEG_normalization_image\n",
    "    global best_avg_RMSE_loss_on_JPEG_normalization_image\n",
    "    global best_depth_range_gradient_L1_losses_by_epoch\n",
    "    global best_max_depth_range_gradient_L1_losses_by_epoch\n",
    "    global best_avg_L1_losses_on_JPEG_normalization_by_epoch\n",
    "    global best_avg_RMSE_losses_on_JPEG_normalization_by_epoch\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Restore the state dictionaries\n",
    "    if config.ignore_mismatched_weight_sizes:\n",
    "        # Use this instead if there are size mismatches in weights, but you want to load the parts of the model that do match\n",
    "        current_model_dict = net.state_dict()\n",
    "        loaded_state_dict = checkpoint['model_state_dict']\n",
    "        new_state_dict = {k:v if v.size() == current_model_dict[k].size()  else  current_model_dict[k] for k,v in zip(current_model_dict.keys(), loaded_state_dict.values())}\n",
    "        net.load_state_dict(new_state_dict, strict=False)    \n",
    "    else:\n",
    "        net.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        if 'scaler_state_dict' in checkpoint and scaler is not None:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    \n",
    "    # Restore other variables\n",
    "    starting_epoch = checkpoint['epoch'] + 1 \n",
    "    global_step = checkpoint['global_step'] + 1\n",
    "    \n",
    "    best_max_depth_range_gradient_L1_loss = checkpoint['best_max_depth_range_gradient_L1_loss']\n",
    "    best_depth_range_gradient_L1_loss = checkpoint['best_depth_range_gradient_L1_loss']\n",
    "    best_avg_L1_loss_on_JPEG_normalization_image = checkpoint['best_avg_L1_loss_on_JPEG_normalization_image']\n",
    "    best_avg_RMSE_loss_on_JPEG_normalization_image = checkpoint['best_avg_RMSE_loss_on_JPEG_normalization_image']\n",
    "    \n",
    "    best_depth_range_gradient_L1_losses_by_epoch = checkpoint['best_depth_range_gradient_L1_losses_by_epoch']\n",
    "    best_max_depth_range_gradient_L1_losses_by_epoch = checkpoint['best_max_depth_range_gradient_L1_losses_by_epoch']\n",
    "    best_avg_L1_losses_on_JPEG_normalization_by_epoch = checkpoint['best_avg_L1_losses_on_JPEG_normalization_by_epoch']\n",
    "    best_avg_RMSE_losses_on_JPEG_normalization_by_epoch = checkpoint['best_avg_RMSE_losses_on_JPEG_normalization_by_epoch']\n",
    "    \n",
    "    print(\"Checkpoint loaded successfully.\")\n",
    "    print(\"Epoch:\", starting_epoch)\n",
    "    print(\"Global step:\", global_step)\n",
    "    print(\"Best Max Depth Range Gradient Loss:\", best_max_depth_range_gradient_L1_loss * 1000)\n",
    "    print(\"Best Depth Range Gradient L1 Loss:\", best_depth_range_gradient_L1_loss * 1000)\n",
    "    print(\"Best Avg L1 Loss on JPEG Normalization Image:\", best_avg_L1_loss_on_JPEG_normalization_image * 1000)\n",
    "    print(\"Best Avg RMSE Loss on JPEG Normalization Image:\", best_avg_RMSE_loss_on_JPEG_normalization_image * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.010612,
     "end_time": "2024-10-07T10:47:47.866219",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.855607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to find the latest epoch checkpoint\n",
    "def find_highest_epoch_checkpoint(weight_save_directory, notebook_name, training_run_number):\n",
    "    # Construct the checkpoint file pattern\n",
    "    epoch_checkpoint_file_path_pattern = f\"{weight_save_directory}{notebook_name}_run{training_run_number}_epoch*.pt\"\n",
    "\n",
    "    # Find all checkpoint files matching the pattern\n",
    "    epoch_checkpoint_files = glob(epoch_checkpoint_file_path_pattern)\n",
    "    \n",
    "    if not epoch_checkpoint_files:\n",
    "        return None\n",
    "    else:\n",
    "        # Extract the epoch numbers from the checkpoint file names\n",
    "        epoch_numbers = []\n",
    "        for epoch_checkpoint_file in epoch_checkpoint_files:\n",
    "            match = re.search(r'epoch(\\d+)', epoch_checkpoint_file)\n",
    "            if match:\n",
    "                epoch_number = int(match.group(1))\n",
    "                epoch_numbers.append(epoch_number)\n",
    "        \n",
    "        # Find the highest epoch number\n",
    "        highest_epoch = max(epoch_numbers)\n",
    "        \n",
    "        # Construct the highest epoch checkpoint file name\n",
    "        highest_epoch_checkpoint = f\"{weight_save_directory}{notebook_name}_run{training_run_number}_epoch{highest_epoch}.pt\"\n",
    "        \n",
    "        return highest_epoch_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009621,
     "end_time": "2024-10-07T10:47:47.881516",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.871895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_images_folder_structure(config, notebook_name):\n",
    "    # Base directory for images\n",
    "    base_dir = \"./images/\" + notebook_name\n",
    "\n",
    "    if config.start_from_scratch and os.path.exists(base_dir) and not (config.dry_run or config.debug_mode):\n",
    "        shutil.rmtree(base_dir)\n",
    "\n",
    "    # List of subdirectories to create\n",
    "    subdirectories = [\n",
    "        \"neural_encoding_for_ones_mask\",\n",
    "        \"neural_encoding_for_zeros_mask\",\n",
    "    ]\n",
    "\n",
    "    # Create each subdirectory if it does not exist\n",
    "    for subdir in subdirectories:\n",
    "        dir_path = os.path.join(base_dir, subdir)\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009984,
     "end_time": "2024-10-07T10:47:47.897753",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.887769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deletes the contents of a directory\n",
    "def clear_directory(directory):\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "            os.unlink(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.01186,
     "end_time": "2024-10-07T10:47:47.915793",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.903933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If training is interrupted, this function finds a recent checkpoint file.  It's also used on subsequent training runs to ascertain the current training run number.\n",
    "def determine_training_run_number(config, save_directory_all_runs):\n",
    "    if not os.path.exists(save_directory_all_runs):\n",
    "        os.makedirs(save_directory_all_runs)\n",
    "\n",
    "    training_run_number = 1\n",
    "    checkpoint_file_path = None\n",
    "\n",
    "    # If starting from scratch, delete all weight files and start over\n",
    "    if config.start_from_scratch:\n",
    "        print(\"Starting from scratch.  Deleting existing runs.\")\n",
    "        clear_directory(save_directory_all_runs)\n",
    "        weight_save_directory = save_directory_all_runs + \"/run_%i/\" % training_run_number\n",
    "        if not config.dry_run:\n",
    "            os.makedirs(weight_save_directory, exist_ok=True)\n",
    "        config_file_path = weight_save_directory + notebook_name + \"_training_configuration_run%i.pt\" % training_run_number\n",
    "    else:\n",
    "        while True:\n",
    "            weight_save_directory = save_directory_all_runs + \"/run_%i/\" % training_run_number\n",
    "            # If the weight save directory exists and is not empty\n",
    "            if os.path.exists(weight_save_directory) and not os.listdir(weight_save_directory) == []:\n",
    "                config_file_path = weight_save_directory + notebook_name + \"_training_configuration_run%i.pt\" % training_run_number\n",
    "                if os.path.exists(config_file_path):\n",
    "                    with open(config_file_path, 'rb') as handle:\n",
    "                        loaded_config = pickle.load(handle)\n",
    "                    if not loaded_config.ran_to_completion:\n",
    "                        potential_checkpoint_file_path_1 = weight_save_directory + notebook_name + \"_run%i_\" % training_run_number + \"best_L1_rangeGradient_val_loss.pt\"\n",
    "                        potential_checkpoint_file_path_2 = find_highest_epoch_checkpoint(weight_save_directory, notebook_name, training_run_number)\n",
    "                        # Preference given to the checkpoint corresponding to the best L1 depth loss on the range gradient image (with no image compression).\n",
    "                        if os.path.exists(potential_checkpoint_file_path_1):\n",
    "                            checkpoint_file_path = potential_checkpoint_file_path_1\n",
    "                        elif os.path.exists(potential_checkpoint_file_path_2):\n",
    "                            checkpoint_file_path = potential_checkpoint_file_path_2\n",
    "                        else:\n",
    "                            raise FileNotFoundError(\"Last run not ran to completion, but pre-trained checkpoint file not found!\")\n",
    "\n",
    "                        # Make sure to start a new run if the precision mode is changing\n",
    "                        if config.precision_mode != config.precision_mode:\n",
    "                            training_run_number += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        if not config.start_from_scratch:\n",
    "                            potential_checkpoint_file_path_1 = weight_save_directory + notebook_name + \"_run%i_\" % training_run_number + \"best_L1_rangeGradient_val_loss.pt\"\n",
    "                            potential_checkpoint_file_path_2 = find_highest_epoch_checkpoint(weight_save_directory, notebook_name, training_run_number)\n",
    "                            if os.path.exists(potential_checkpoint_file_path_1):\n",
    "                                checkpoint_file_path = potential_checkpoint_file_path_1\n",
    "                            elif os.path.exists(potential_checkpoint_file_path_2):\n",
    "                                checkpoint_file_path = potential_checkpoint_file_path_2\n",
    "                            else:\n",
    "                                raise FileNotFoundError(\"Pre-trained checkpoint file not found!\")\n",
    "                        training_run_number += 1\n",
    "                else:\n",
    "                    print(config_file_path)\n",
    "                    print(\"Couldn't find config file \" + config_file_path + \", so starting this run over from scratch\")\n",
    "                    break\n",
    "            # If the weight save directory doesn't exist or is empty\n",
    "            else:\n",
    "                if not config.dry_run:\n",
    "                    os.makedirs(weight_save_directory, exist_ok=True)\n",
    "                break\n",
    "    \n",
    "            # Prevent a runaway loop in case of any issues\n",
    "            if training_run_number >= 10:\n",
    "                raise Exception(\"Training run number greater than 10 seems to indicate an issue in determine_training_run_number()\")\n",
    "                \n",
    "    return training_run_number, weight_save_directory, config, checkpoint_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009142,
     "end_time": "2024-10-07T10:47:47.931136",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.921994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This simply allows the form of differentiable rounding to be easily configurable\n",
    "def diff_round_verbose(x, differentiable_rounding_type = QuantizerProxyType.STRAIGHT_THROUGH):\n",
    "    if differentiable_rounding_type == QuantizerProxyType.SOFT:\n",
    "        return torch.round(x) + (x - torch.round(x))**3\n",
    "    elif differentiable_rounding_type == QuantizerProxyType.STRAIGHT_THROUGH:\n",
    "        return x + (torch.round(x) - x).detach()\n",
    "\n",
    "# Make sure to use this instead of real rounding when training\n",
    "diff_round = partial(diff_round_verbose, differentiable_rounding_type=config.differentiable_rounding_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005731,
     "end_time": "2024-10-07T10:47:47.942665",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.936934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### DiffJPEG Functions (lightly modified from https://github.com/mlomnitz/DiffJPEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005819,
     "end_time": "2024-10-07T10:47:47.954241",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.948422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### DiffJPEG License:\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2021 Michael R Lomnitz\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.011553,
     "end_time": "2024-10-07T10:47:47.971586",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.960033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From utils.py of DiffJPEG\n",
    "\n",
    "y_table = np.array(\n",
    "    [[16, 11, 10, 16, 24, 40, 51, 61],\n",
    "     [12, 12, 14, 19, 26, 58, 60, 55], \n",
    "     [14, 13, 16, 24, 40, 57, 69, 56],\n",
    "     [14, 17, 22, 29, 51, 87, 80, 62],\n",
    "     [18, 22, 37, 56, 68, 109, 103, 77],\n",
    "     [24, 35, 55, 64, 81, 104, 113, 92],\n",
    "     [49, 64, 78, 87, 103, 121, 120, 101],\n",
    "     [72, 92, 95, 98, 112, 100, 103, 99]],\n",
    "    dtype=np.float32).T\n",
    "y_table = nn.Parameter(torch.from_numpy(y_table))\n",
    "\n",
    "c_table = np.empty((8, 8), dtype=np.float32)\n",
    "c_table.fill(99)\n",
    "c_table[:4, :4] = np.array([[17, 18, 24, 47],\n",
    "                            [18, 21, 26, 66],\n",
    "                            [24, 26, 56, 99],\n",
    "                            [47, 66, 99, 99]]).T\n",
    "c_table = nn.Parameter(torch.from_numpy(c_table))\n",
    "\n",
    "def quality_to_factor(quality):\n",
    "    \"\"\" Calculate factor corresponding to quality\n",
    "    Input:\n",
    "        quality(float): Quality for jpeg compression\n",
    "    Output:\n",
    "        factor(float): Compression factor\n",
    "    \"\"\"\n",
    "    if quality < 50:\n",
    "        quality = 5000. / quality\n",
    "    else:\n",
    "        quality = 200. - quality*2\n",
    "    return quality / 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.017457,
     "end_time": "2024-10-07T10:47:47.995101",
     "exception": false,
     "start_time": "2024-10-07T10:47:47.977644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From compression.py of DiffJPEG\n",
    "\n",
    "class rgb_to_ycbcr_jpeg(nn.Module):\n",
    "    \"\"\" Converts RGB image to YCbCr\n",
    "    Input:\n",
    "        image(tensor): batch x 3 x height x width\n",
    "    Outpput:\n",
    "        result(tensor): batch x height x width x 3\n",
    "    \"\"\"\n",
    "    def __init__(self, color_space_conversions):  # Modification to DiffJPEG\n",
    "        super(rgb_to_ycbcr_jpeg, self).__init__()\n",
    "        matrix = np.array(\n",
    "            [[0.299, 0.587, 0.114], \n",
    "             [-0.168736, -0.331264, 0.5],\n",
    "             [0.5, -0.418688, -0.081312]], dtype=np.float32).T\n",
    "        self.shift = nn.Parameter(torch.tensor([0., 128., 128.]))\n",
    "\n",
    "        self.matrix = nn.Parameter(torch.from_numpy(matrix))\n",
    "        self.color_space_conversions = color_space_conversions  # Modification to DiffJPEG\n",
    "\n",
    "    def forward(self, image):\n",
    "        result = image.permute(0, 2, 3, 1)\n",
    "        \n",
    "        if self.color_space_conversions:  # Modification to DiffJPEG\n",
    "            result = torch.tensordot(result, self.matrix, dims=1) + self.shift\n",
    "            result.view(image.shape)\n",
    "        \n",
    "        return result\n",
    "\n",
    "class chroma_subsampling(nn.Module):\n",
    "    \"\"\" Chroma subsampling on CbCv channels\n",
    "    Input:\n",
    "        image(tensor): batch x height x width x 3\n",
    "    Output:\n",
    "        y(tensor): batch x height x width\n",
    "        cb(tensor): batch x height/2 x width/2\n",
    "        cr(tensor): batch x height/2 x width/2\n",
    "    \"\"\"\n",
    "    def __init__(self, subsampling):\n",
    "        super(chroma_subsampling, self).__init__()  # Modification to DiffJPEG\n",
    "        self.subsampling = subsampling  # Modification to DiffJPEG\n",
    "\n",
    "    def forward(self, image):\n",
    "        if self.subsampling:  # Modification to DiffJPEG\n",
    "            image_2 = image.permute(0, 3, 1, 2).clone()\n",
    "            avg_pool = nn.AvgPool2d(kernel_size=2, stride=(2, 2),\n",
    "                                    count_include_pad=False)\n",
    "            cb = avg_pool(image_2[:, 1, :, :].unsqueeze(1))\n",
    "            cr = avg_pool(image_2[:, 2, :, :].unsqueeze(1))\n",
    "            cb = cb.permute(0, 2, 3, 1)\n",
    "            cr = cr.permute(0, 2, 3, 1)\n",
    "            \n",
    "            return image[:, :, :, 0], cb, cr\n",
    "        else:\n",
    "            return image[:, :, :, 0], image[:, :, :, 1], image[:, :, :, 2]  # Modification to DiffJPEG\n",
    "\n",
    "\n",
    "class block_splitting(nn.Module):\n",
    "    \"\"\" Splitting image into patches\n",
    "    Input:\n",
    "        image(tensor): batch x height x width\n",
    "    Output: \n",
    "        patch(tensor):  batch x h*w/64 x h x w\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(block_splitting, self).__init__()\n",
    "        self.k = 8\n",
    "\n",
    "    def forward(self, image):\n",
    "        height, width = image.shape[1:3]\n",
    "        batch_size = image.shape[0]\n",
    "        image_reshaped = image.view(batch_size, height // self.k, self.k, -1, self.k)\n",
    "        image_transposed = image_reshaped.permute(0, 1, 3, 2, 4)\n",
    "        return image_transposed.contiguous().view(batch_size, -1, self.k, self.k)\n",
    "    \n",
    "\n",
    "class dct_8x8(nn.Module):\n",
    "    \"\"\" Discrete Cosine Transformation\n",
    "    Input:\n",
    "        image(tensor): batch x height x width\n",
    "    Output:\n",
    "        dcp(tensor): batch x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(dct_8x8, self).__init__()\n",
    "        tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n",
    "        for x, y, u, v in itertools.product(range(8), repeat=4):\n",
    "            tensor[x, y, u, v] = np.cos((2 * x + 1) * u * np.pi / 16) * np.cos(\n",
    "                (2 * y + 1) * v * np.pi / 16)\n",
    "        alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n",
    "        #\n",
    "        self.tensor =  nn.Parameter(torch.from_numpy(tensor).float())\n",
    "        self.scale = nn.Parameter(torch.from_numpy(np.outer(alpha, alpha) * 0.25).float() )\n",
    "        \n",
    "    def forward(self, image):\n",
    "        image = image - 128\n",
    "        result = self.scale * torch.tensordot(image, self.tensor, dims=2)\n",
    "        result.view(image.shape)\n",
    "        return result\n",
    "\n",
    "\n",
    "class y_quantize(nn.Module):\n",
    "    \"\"\" JPEG Quantization for Y channel\n",
    "    Input:\n",
    "        image(tensor): batch x height x width\n",
    "        rounding(function): rounding function to use\n",
    "        factor(float): Degree of compression\n",
    "    Output:\n",
    "        image(tensor): batch x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(y_quantize, self).__init__()\n",
    "        self.y_table = y_table\n",
    "\n",
    "    def forward(self, image, factor, rounding):  # Modification to DiffJPEG\n",
    "        image = image.float() / (self.y_table * factor)\n",
    "        image = rounding(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "class c_quantize(nn.Module):\n",
    "    \"\"\" JPEG Quantization for CrCb channels\n",
    "    Input:\n",
    "        image(tensor): batch x height x width\n",
    "        rounding(function): rounding function to use\n",
    "        factor(float): Degree of compression\n",
    "    Output:\n",
    "        image(tensor): batch x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(c_quantize, self).__init__()\n",
    "        self.c_table = c_table\n",
    "\n",
    "    def forward(self, image, factor, rounding):  # Modification to DiffJPEG\n",
    "        image = image.float() / (self.c_table * factor)\n",
    "        image = rounding(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "class compress_jpeg(nn.Module):\n",
    "    \"\"\" Full JPEG compression algortihm\n",
    "    Input:\n",
    "        imgs(tensor): batch x 3 x height x width\n",
    "        rounding(function): rounding function to use\n",
    "    Ouput:\n",
    "        compressed(dict(tensor)): batch x h*w/64 x 8 x 8\n",
    "    \"\"\"\n",
    "    def __init__(self, subsampling=True, color_space_conversions=True):  # Modification to DiffJPEG\n",
    "        super(compress_jpeg, self).__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            rgb_to_ycbcr_jpeg(color_space_conversions),\n",
    "            chroma_subsampling(subsampling)\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            block_splitting(),\n",
    "            dct_8x8()\n",
    "        )\n",
    "        self.c_quantize = c_quantize()\n",
    "        self.y_quantize = y_quantize()\n",
    "        self.subsampling = subsampling  # Modification to DiffJPEG\n",
    "\n",
    "    def forward(self, image, factor, rounding):\n",
    "        y, cb, cr = self.l1(image*255)\n",
    "        components = {'y': y, 'cb': cb, 'cr': cr}\n",
    "        for k in components.keys():\n",
    "            comp = self.l2(components[k])\n",
    "            if k in ('cb', 'cr') and self.subsampling:  # Modification to DiffJPEG\n",
    "                comp = self.c_quantize(comp, factor, rounding)  # Modification to DiffJPEG\n",
    "            else:\n",
    "                comp = self.y_quantize(comp, factor, rounding)  # Modification to DiffJPEG\n",
    "\n",
    "            components[k] = comp\n",
    "\n",
    "        return components['y'], components['cb'], components['cr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.017999,
     "end_time": "2024-10-07T10:47:48.020136",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.002137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From decompression.py of DiffJPEG\n",
    "\n",
    "class y_dequantize(nn.Module):\n",
    "    \"\"\" Dequantize Y channel\n",
    "    Inputs:\n",
    "        image(tensor): batch x height x width\n",
    "    Outputs:\n",
    "        image(tensor): batch x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(y_dequantize, self).__init__()\n",
    "        self.y_table = y_table\n",
    "\n",
    "    def forward(self, image, factor):  # Modification to DiffJPEG\n",
    "        return image * (self.y_table * factor)\n",
    "\n",
    "\n",
    "class c_dequantize(nn.Module):\n",
    "    \"\"\" Dequantize CbCr channel\n",
    "    Inputs:\n",
    "        image(tensor): batch x height x width\n",
    "    Outputs:\n",
    "        image(tensor): batch x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(c_dequantize, self).__init__()\n",
    "        self.c_table = c_table\n",
    "\n",
    "    def forward(self, image, factor):  # Modification to DiffJPEG\n",
    "        return image * (self.c_table * factor)\n",
    "\n",
    "\n",
    "class idct_8x8(nn.Module):\n",
    "    \"\"\" Inverse discrete Cosine Transformation\n",
    "    Input:\n",
    "        dcp(tensor): batch x height x width\n",
    "    Output:\n",
    "        image(tensor): batch x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(idct_8x8, self).__init__()\n",
    "        alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n",
    "        self.alpha = nn.Parameter(torch.from_numpy(np.outer(alpha, alpha)).float())\n",
    "        tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n",
    "        for x, y, u, v in itertools.product(range(8), repeat=4):\n",
    "            tensor[x, y, u, v] = np.cos((2 * u + 1) * x * np.pi / 16) * np.cos(\n",
    "                (2 * v + 1) * y * np.pi / 16)\n",
    "        self.tensor = nn.Parameter(torch.from_numpy(tensor).float())\n",
    "\n",
    "    def forward(self, image):\n",
    "        image = image * self.alpha\n",
    "        result = 0.25 * torch.tensordot(image, self.tensor, dims=2) + 128\n",
    "        result.view(image.shape)\n",
    "        return result\n",
    "\n",
    "\n",
    "class block_merging(nn.Module):\n",
    "    \"\"\" Merge pathces into image\n",
    "    Inputs:\n",
    "        patches(tensor) batch x height*width/64, height x width\n",
    "        height(int)\n",
    "        width(int)\n",
    "    Output:\n",
    "        image(tensor): batch x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(block_merging, self).__init__()\n",
    "        \n",
    "    def forward(self, patches, height, width):\n",
    "        k = 8\n",
    "        batch_size = patches.shape[0]\n",
    "        image_reshaped = patches.view(batch_size, height//k, width//k, k, k)\n",
    "        image_transposed = image_reshaped.permute(0, 1, 3, 2, 4)\n",
    "        return image_transposed.contiguous().view(batch_size, height, width)\n",
    "\n",
    "\n",
    "class chroma_upsampling(nn.Module):\n",
    "    \"\"\" Upsample chroma layers\n",
    "    Input: \n",
    "        y(tensor): y channel image\n",
    "        cb(tensor): cb channel\n",
    "        cr(tensor): cr channel\n",
    "    Ouput:\n",
    "        image(tensor): batch x height x width x 3\n",
    "    \"\"\"\n",
    "    def __init__(self, subsampling):\n",
    "        super(chroma_upsampling, self).__init__()\n",
    "        self.subsampling = subsampling\n",
    "\n",
    "    def forward(self, y, cb, cr):\n",
    "        if self.subsampling:  # Modification to DiffJPEG\n",
    "#             def repeat(x, k=2):\n",
    "#                 height, width = x.shape[1:3]\n",
    "#                 x = x.unsqueeze(-1)\n",
    "#                 x = x.repeat(1, 1, k, k)\n",
    "#                 x = x.view(-1, height * k, width * k)\n",
    "#                 return x\n",
    "\n",
    "#             cb = repeat(cb)\n",
    "#             cr = repeat(cr)\n",
    "            cb = torch.squeeze(nn.Upsample(scale_factor=2, mode='bilinear')(torch.unsqueeze(cb, 0)), 0)  # Modification to DiffJPEG\n",
    "            cr = torch.squeeze(nn.Upsample(scale_factor=2, mode='bilinear')(torch.unsqueeze(cr, 0)), 0)  # Modification to DiffJPEG\n",
    "        \n",
    "        return torch.cat([y.unsqueeze(3), cb.unsqueeze(3), cr.unsqueeze(3)], dim=3)\n",
    "\n",
    "\n",
    "class ycbcr_to_rgb_jpeg(nn.Module):\n",
    "    \"\"\" Converts YCbCr image to RGB JPEG\n",
    "    Input:\n",
    "        image(tensor): batch x height x width x 3\n",
    "    Outpput:\n",
    "        result(tensor): batch x 3 x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self, color_space_conversions=True):  # Modification to DiffJPEG\n",
    "        super(ycbcr_to_rgb_jpeg, self).__init__()\n",
    "\n",
    "        matrix = np.array(\n",
    "            [[1., 0., 1.402], [1, -0.344136, -0.714136], [1, 1.772, 0]],\n",
    "            dtype=np.float32).T\n",
    "        self.shift = nn.Parameter(torch.tensor([0, -128., -128.]))\n",
    "        self.matrix = nn.Parameter(torch.from_numpy(matrix))\n",
    "        \n",
    "        self.color_space_conversions = color_space_conversions  # Modification to DiffJPEG\n",
    "\n",
    "    def forward(self, image):\n",
    "        if self.color_space_conversions:  # Modification to DiffJPEG\n",
    "            result = torch.tensordot(image + self.shift, self.matrix, dims=1)\n",
    "            result.view(image.shape)\n",
    "        else:\n",
    "            result = image  # Modification to DiffJPEG\n",
    "\n",
    "        return result.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class decompress_jpeg(nn.Module):\n",
    "    \"\"\" Full JPEG decompression algortihm\n",
    "    Input:\n",
    "        compressed(dict(tensor)): batch x h*w/64 x 8 x 8\n",
    "    Ouput:\n",
    "        image(tensor): batch x 3 x height x width\n",
    "    \"\"\"\n",
    "    def __init__(self, height, width, subsampling=True, color_space_conversions=True):  # Modification to DiffJPEG\n",
    "        super(decompress_jpeg, self).__init__()\n",
    "        self.c_dequantize = c_dequantize()\n",
    "        self.y_dequantize = y_dequantize()\n",
    "        self.idct = idct_8x8()\n",
    "        self.merging = block_merging()\n",
    "        self.chroma = chroma_upsampling(subsampling)  # Modification to DiffJPEG\n",
    "        self.colors = ycbcr_to_rgb_jpeg(color_space_conversions)  # Modification to DiffJPEG\n",
    "        \n",
    "        self.height, self.width = height, width\n",
    "        self.subsampling = subsampling\n",
    "        \n",
    "    def forward(self, y, cb, cr, factor, rounding):\n",
    "        components = {'y': y, 'cb': cb, 'cr': cr}\n",
    "        for k in components.keys():\n",
    "            if k in ('cb', 'cr') and self.subsampling:  # Modification to DiffJPEG\n",
    "                comp = self.c_dequantize(components[k], factor)  # Modification to DiffJPEG\n",
    "                height, width = int(self.height/2), int(self.width/2)    \n",
    "            else:  # Modification to DiffJPEG\n",
    "                comp = self.y_dequantize(components[k], factor)  # Modification to DiffJPEG\n",
    "                height, width = self.height, self.width  # Modification to DiffJPEG\n",
    "            comp = self.idct(comp)\n",
    "            components[k] = self.merging(comp, height, width)\n",
    "            #\n",
    "        image = self.chroma(components['y'], components['cb'], components['cr'])\n",
    "        image = self.colors(image)\n",
    "        \n",
    "        image = torch.min(255*torch.ones_like(image), torch.max(torch.zeros_like(image), image))\n",
    "#         return image/255  # Modification to DiffJPEG\n",
    "        image = rounding(image)  # Modification to DiffJPEG\n",
    "        \n",
    "        return image/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.0108,
     "end_time": "2024-10-07T10:47:48.037929",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.027129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DiffJPEG with some modifications:\n",
    "#     -subsampling: True = 4:2:0 chroma subsampling, False = 4:4:4 chroma sampling (i.e., no chroma subsampling)\n",
    "#     -color_space_conversions: True = RGB->YCbCr and YCbCr->RGB color space conversions (typical JPEG behavior), False = Input image expected to be in YCbCr, so no color space conversions performed\n",
    "#     -forward() function modified to allow for dynamic changes of the JPEG quality and whether or not differentiable rounding is enabled\n",
    "#     -Under the hood, the chroma upsampling operation was also corrected to use bilinear upsampling (much closer to a typical JPEG implementation).\n",
    "class DiffJPEG(nn.Module):\n",
    "    def __init__(self, height, width, subsampling=True, color_space_conversions=True):\n",
    "        ''' Initialize the DiffJPEG layer\n",
    "        Inputs:\n",
    "            height(int): Original image hieght\n",
    "            width(int): Original image width\n",
    "            differentiable(bool): If true uses custom differentiable\n",
    "                rounding function, if false uses standrard torch.round\n",
    "            quality(float): Quality factor for jpeg compression scheme. \n",
    "        '''\n",
    "        super(DiffJPEG, self).__init__()\n",
    "        self.compress = compress_jpeg(subsampling=subsampling, color_space_conversions=color_space_conversions)\n",
    "        self.decompress = decompress_jpeg(height, width, subsampling=subsampling, color_space_conversions=color_space_conversions)\n",
    "\n",
    "    def forward(self, x, quality, differentiable):\n",
    "        rounding = None\n",
    "        if differentiable:\n",
    "            rounding = diff_round\n",
    "        else:\n",
    "            rounding = torch.round\n",
    "        \n",
    "        factor = quality_to_factor(quality)\n",
    "        y, cb, cr = self.compress(x, factor, rounding)\n",
    "        recovered = self.decompress(y, cb, cr, factor, rounding)\n",
    "        return recovered, y, cb, cr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.006259,
     "end_time": "2024-10-07T10:47:48.050196",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.043937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## FlyingThings3D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.015693,
     "end_time": "2024-10-07T10:47:48.072336",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.056643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_read_pfm_file = partial(_read_pfm, slice_channels=1)  # For reading PFM (Portable FloatMap) files from FlyingThings3D dataset\n",
    "\n",
    "# Modified version of a PyTorch dataloader\n",
    "class FlyingThingsDataset(ABC, VisionDataset):\n",
    "    \"\"\"Base interface for Stereo matching datasets\"\"\"\n",
    "\n",
    "    _has_built_in_disparity_mask = False\n",
    "\n",
    "    def __init__(self, root: str, train_or_test: str, random_crops, transforms: Optional[Callable] = None) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root(str): Root directory of the dataset.\n",
    "            transforms(callable, optional): A function/transform that takes in Tuples of\n",
    "                (images, disparities, valid_masks) and returns a transformed version of each of them.\n",
    "                images is a Tuple of (``PIL.Image``, ``PIL.Image``)\n",
    "                disparities is a Tuple of (``np.ndarray``, ``np.ndarray``) with shape (1, H, W)\n",
    "                valid_masks is a Tuple of (``np.ndarray``, ``np.ndarray``) with shape (H, W)\n",
    "                In some cases, when a dataset does not provide disparities, the ``disparities`` and\n",
    "                ``valid_masks`` can be Tuples containing None values.\n",
    "                For training splits generally the datasets provide a minimal guarantee of\n",
    "                images: (``PIL.Image``, ``PIL.Image``)\n",
    "                disparities: (``np.ndarray``, ``None``) with shape (1, H, W)\n",
    "                Optionally, based on the dataset, it can return a ``mask`` as well:\n",
    "                valid_masks: (``np.ndarray | None``, ``None``) with shape (H, W)\n",
    "                For some test splits, the datasets provides outputs that look like:\n",
    "                imgaes: (``PIL.Image``, ``PIL.Image``)\n",
    "                disparities: (``None``, ``None``)\n",
    "                Optionally, based on the dataset, it can return a ``mask`` as well:\n",
    "                valid_masks: (``None``, ``None``)\n",
    "        \"\"\"\n",
    "        super().__init__(root=root)\n",
    "        self.transforms = transforms  # You can load up the transforms argument with you data augmentation\n",
    "\n",
    "        self._images = []  # type: ignore\n",
    "        self._disparities = []  # type: ignore\n",
    "\n",
    "        self.train_or_test = train_or_test\n",
    "        self.random_crops = random_crops\n",
    "        \n",
    "        root = Path(root)\n",
    "        \n",
    "        left_disparity_pattern = str(root / \"FlyingThings3D/disparity\" / train_or_test / \"*/*/left/*.pfm\")\n",
    "        #left_image_pattern = str(root / \"FlyingThings3D\" / \"*\" / \"*\" / \"*\" / \"left\" / \"*.png\")\n",
    "        right_disparity_pattern = str(root / \"FlyingThings3D/disparity\" / train_or_test / \"*/*/right/*.pfm\") \n",
    "        \n",
    "        self._disparities += self._scan_pairs(left_disparity_pattern, right_disparity_pattern)\n",
    "\n",
    "    def _read_img(self, file_path: Union[str, Path]) -> Image.Image:\n",
    "        img = Image.open(file_path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def _scan_pairs(self, paths_left_pattern: str, paths_right_pattern: Optional[str] = None,\n",
    "    ) -> List[Tuple[str, Optional[str]]]:\n",
    "        left_paths = list(sorted(glob(paths_left_pattern)))\n",
    "\n",
    "        right_paths: List[Union[None, str]]\n",
    "        if paths_right_pattern:\n",
    "            right_paths = list(sorted(glob(paths_right_pattern)))\n",
    "        else:\n",
    "            right_paths = list(None for _ in left_paths)\n",
    "        \n",
    "        if not left_paths:\n",
    "            raise FileNotFoundError(f\"Could not find any files matching the patterns: {paths_left_pattern}\")\n",
    "\n",
    "        if not right_paths:\n",
    "            raise FileNotFoundError(f\"Could not find any files matching the patterns: {paths_right_pattern}\")\n",
    "\n",
    "        if len(left_paths) != len(right_paths):\n",
    "            raise ValueError(\n",
    "                f\"Found {len(left_paths)} left files but {len(right_paths)} right files using:\\n \"\n",
    "                f\"left pattern: {paths_left_pattern}\\n\"\n",
    "                f\"right pattern: {paths_right_pattern}\\n\"\n",
    "            )\n",
    "\n",
    "        paths = list((left, right) for left, right in zip(left_paths, right_paths))\n",
    "        return paths\n",
    "    \n",
    "    def _read_disparity(self, file_path: str) -> Tuple[np.ndarray, None]:\n",
    "        disparity_map = _read_pfm_file(file_path)\n",
    "        disparity_map = np.abs(disparity_map)  # Ensure that the disparity is positive\n",
    "        valid_mask = None\n",
    "        return disparity_map\n",
    "\n",
    "    def __getitem__(self, index: int) -> np.ndarray:\n",
    "        \"\"\"Return example at given index.\n",
    "\n",
    "        Args:\n",
    "            index(int): The index of the example to retrieve\n",
    "\n",
    "        Returns:\n",
    "            tuple: A 3 or 4-tuple with ``(img_left, img_right, disparity, Optional[valid_mask])`` where ``valid_mask``\n",
    "                can be a numpy boolean mask of shape (H, W) if the dataset provides a file\n",
    "                indicating which disparity pixels are valid. The disparity is a numpy array of\n",
    "                shape (1, H, W) and the images are PIL images. ``disparity`` is None for\n",
    "                datasets on which for ``split=\"test\"`` the authors did not provide annotations.\n",
    "        \"\"\"\n",
    "                \n",
    "        if random.random() < 0.5:\n",
    "            dsp_map = self._read_disparity(self._disparities[index][0])\n",
    "        else:\n",
    "            dsp_map = self._read_disparity(self._disparities[index][1])\n",
    "        \n",
    "        #dsp_maps = (dsp_map_left, dsp_map_right)\n",
    "        dsp_maps = torch.from_numpy(dsp_map)\n",
    "\n",
    "        if self.random_crops:\n",
    "            x_crop_ratio = random.uniform(0.16875, 1.0)\n",
    "            y_crop_ratio = random.uniform(0.3, 1.0)\n",
    "            crop_size = (int(540 * y_crop_ratio), int(960 * x_crop_ratio))\n",
    "                \n",
    "            # Random crop\n",
    "            i, j, h, w = T.RandomCrop.get_params(dsp_maps, output_size=crop_size)\n",
    "            dsp_maps = T.functional.crop(dsp_maps, i, j, h, w)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            dsp_maps = self.transforms(dsp_maps)\n",
    "        \n",
    "        dsp_maps[dsp_maps <= 0] = 1e-6  # In case there are any disparities that equal 0, we need to make them a small positive value so that depth is never calculated as NaN (for depth normalization purposes)\n",
    "            \n",
    "        depth_maps = 1 / dsp_maps\n",
    "\n",
    "        return depth_maps\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._disparities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.005952,
     "end_time": "2024-10-07T10:47:48.084684",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.078732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Neural Depth Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.016903,
     "end_time": "2024-10-07T10:47:48.107453",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.090550",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralDepthAutoencoder(nn.Module):\n",
    "    def __init__(self, config, height, width, subsampling=True, color_space_conversions=True, device=torch.device(\"cuda\")):\n",
    "        super(NeuralDepthAutoencoder, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Save a copy of relevant config variables to the model object\n",
    "        self.update_config(config)\n",
    "                \n",
    "        # ENCODER\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(2, self.neural_encoder_width, kernel_size=1, stride=1, padding=0),\n",
    "            self.encoder_activation_function,\n",
    "            nn.Conv2d(self.neural_encoder_width, self.neural_encoder_width // 2, kernel_size=1, stride=1, padding=0),\n",
    "            self.encoder_activation_function,\n",
    "            nn.Conv2d(self.neural_encoder_width // 2, self.neural_encoder_width // 4, kernel_size=1, stride=1, padding=0),\n",
    "            self.encoder_activation_function,\n",
    "            nn.Conv2d(self.neural_encoder_width // 4, self.neural_encoder_width // 4, kernel_size=1, stride=1, padding=0),\n",
    "            self.encoder_activation_function,\n",
    "            nn.Conv2d(self.neural_encoder_width // 4, self.neural_encoder_width // 4, kernel_size=1, stride=1, padding=0),\n",
    "            self.encoder_activation_function,\n",
    "            nn.Conv2d(self.neural_encoder_width // 4, self.neural_encoder_width // 4, kernel_size=1, stride=1, padding=0),\n",
    "            self.encoder_activation_function,\n",
    "            nn.Conv2d(self.neural_encoder_width // 4, self.neural_encoder_width // 32, kernel_size=1, stride=1, padding=0),\n",
    "            self.encoder_activation_function,\n",
    "            nn.Conv2d(self.neural_encoder_width // 32, 3, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        # DECODER\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(3, self.neural_decoder_width, kernel_size=1, stride=1, padding=0),\n",
    "            self.decoder_activation_function,\n",
    "            nn.Conv2d(self.neural_decoder_width, self.neural_decoder_width // 2, kernel_size=1, stride=1, padding=0),\n",
    "            self.decoder_activation_function,\n",
    "            nn.Conv2d(self.neural_decoder_width // 2, self.neural_decoder_width // 4, kernel_size=1, stride=1, padding=0),\n",
    "            self.decoder_activation_function,\n",
    "            nn.Conv2d(self.neural_decoder_width // 4, self.neural_decoder_width // 4, kernel_size=1, stride=1, padding=0),\n",
    "            self.decoder_activation_function,\n",
    "            nn.Conv2d(self.neural_decoder_width // 4, self.neural_decoder_width // 4, kernel_size=1, stride=1, padding=0),\n",
    "            self.decoder_activation_function,\n",
    "            nn.Conv2d(self.neural_decoder_width // 4, self.neural_decoder_width // 4, kernel_size=1, stride=1, padding=0),\n",
    "            self.decoder_activation_function,\n",
    "            nn.Conv2d(self.neural_decoder_width // 4, self.neural_decoder_width // 32, kernel_size=1, stride=1, padding=0),\n",
    "            self.decoder_activation_function,\n",
    "            nn.Conv2d(self.neural_decoder_width // 32, 2, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "        self.JPEG = DiffJPEG(height, width, subsampling, color_space_conversions)\n",
    "\n",
    "        # Weight Freezing\n",
    "        self.freeze_weights(config)\n",
    "        self.JPEG.requires_grad_(False)  # VERY IMPORTANT TO INCLUDE: DiffJPEG includes some parameters that could be \"trained\", which would give the illusion of good results.\n",
    "\n",
    "    def update_config(self, config):\n",
    "        self.neural_encoder_width = config.neural_encoder_width\n",
    "        self.encoder_activation_function = config.encoder_activation_function\n",
    "        self.final_encoder_activation_function = config.final_encoder_activation_function\n",
    "\n",
    "        self.neural_decoder_width = config.neural_decoder_width\n",
    "        self.decoder_activation_function = config.decoder_activation_function\n",
    "        \n",
    "        self.depth_loss_enabled = config.depth_loss_enabled\n",
    "        self.mask_loss_enabled = config.mask_loss_enabled\n",
    "\n",
    "        self.freeze_encoder = config.freeze_encoder\n",
    "        self.freeze_decoder = config.freeze_decoder\n",
    "    \n",
    "    def freeze_weights(self, config):\n",
    "        self.update_config(config)\n",
    "        \n",
    "        self.encoder.requires_grad_(not self.freeze_encoder)\n",
    "        self.decoder.requires_grad_(not self.freeze_decoder)\n",
    "    \n",
    "    def forward(self, x, quality, differentiable_rounding=True, image_compression_type=ImageCompressionType.DIFF_JPEG, perform_encoding=True, perform_decoding=True):\n",
    "        # For downstream convenience during testing (where video frames may be being used instead of still images), both the encoding and decoding steps can be enabled/disabled in the forward() pass\n",
    "        if perform_encoding:\n",
    "            # Split apart the normalized depth input and the mask layer (1's where the \"foreground\" is and 0's for the \"background\")\n",
    "            z_norm, input_mask = x.split(1, dim=1)\n",
    "\n",
    "            # Depth + Mask layers => Color Pixels\n",
    "            encoding = self.encoder(x)\n",
    "\n",
    "            # Apply an activation function to ensure [0,1] range\n",
    "            if self.final_encoder_activation_function is FinalEncoderActivationFunction.SINE:\n",
    "                encoding = (torch.sin(2 * torch.pi * encoding) + 1) / 2  # Sine activation with [0,1]-normalized range\n",
    "            elif self.final_encoder_activation_function is FinalEncoderActivationFunction.SIGMOID:\n",
    "                encoding = nn.Sigmoid()(encoding)\n",
    "            elif self.final_encoder_activation_function is FinalEncoderActivationFunction.NONE:\n",
    "                # Excluding this without enforcing a [0,1] output range elsewhere in the model will result in incompatibility with DiffJPEG\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"Invalid final encoder activation function selected!\")\n",
    "\n",
    "            # Differentiable rounding should be used during training, but not during testing or validation.\n",
    "            if differentiable_rounding:\n",
    "                encoding = diff_round(encoding * 255) / 255\n",
    "            else:\n",
    "                encoding = torch.round(encoding * 255) / 255\n",
    "\n",
    "            # JPEG or DiffJPEG compression (if enabled, otherwise \"lossless image compression\")\n",
    "            # The actual JPEG codec option should be used during testing.\n",
    "            JPEG_compressed = None\n",
    "            if image_compression_type == ImageCompressionType.DIFF_JPEG:\n",
    "                JPEG_compressed, y, cb, cr = self.JPEG(encoding, quality, differentiable_rounding)\n",
    "                JPEG_size = None  # Not calculated since the lossless compression parts of the JPEG algorithm are not simulated\n",
    "            elif image_compression_type == ImageCompressionType.JPEG:\n",
    "                img = toPILImage(encoding.squeeze())\n",
    "                jpegInMemory = BytesIO()\n",
    "                if self.subsampling:\n",
    "                    img.save(jpegInMemory, quality=int(round(quality)), format=\"jpeg\", subsampling=2)  # 4:2:0 chroma subsampling\n",
    "                else:\n",
    "                    img.save(jpegInMemory, quality=int(round(quality)), format=\"jpeg\", subsampling=0)  # 4:4:4 (no chroma subsampling)\n",
    "                JPEG_size = jpegInMemory.getbuffer().nbytes  # JPEG size in bytes\n",
    "                JPEG_compressed = torch.reshape(toTensor(Image.open(jpegInMemory)), [1, 3, x.shape[2], x.shape[3]]).to(device)\n",
    "                y, cb, cr = None, None, None\n",
    "            elif image_compression_type == ImageCompressionType.LOSSLESS:\n",
    "                JPEG_compressed = encoding  # \"Lossless compression\".  A bit of a misnomer in terms of variable names if JPEG compression is not enabled.\n",
    "                y, cb, cr = None, None, None\n",
    "                JPEG_size = None\n",
    "            else:\n",
    "                print(image_compression_type)\n",
    "                raise ValueError(\"Invalid image compression type selected!\")\n",
    "        else:\n",
    "            JPEG_compressed = x\n",
    "            encoding = None\n",
    "            y, cb, cr = None, None, None\n",
    "            JPEG_size = None\n",
    "        \n",
    "        if perform_decoding and (self.depth_loss_enabled or self.mask_loss_enabled):\n",
    "            recovered_depth_and_mask = self.decoder(JPEG_compressed)\n",
    "            recovered_depth = recovered_depth_and_mask[:, 0, :, :].reshape([x.shape[0], 1, x.shape[2], x.shape[3]])\n",
    "            recovered_mask = recovered_depth_and_mask[:, 1, :, :].reshape([x.shape[0], 1, x.shape[2], x.shape[3]])\n",
    "        else:\n",
    "            recovered_depth = None\n",
    "            recovered_mask = None\n",
    "    \n",
    "        return recovered_depth, recovered_mask, encoding, y, cb, cr, JPEG_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.01285,
     "end_time": "2024-10-07T10:47:48.126637",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.113787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I didn't use a rate-distortion loss, so I periodically calculated some normalization factors to approximately equalize losses across different JPEG quality levels.\n",
    "# I elected to use differentiable rounding here, but it wasn't a very important choice.  Not really relevant to how you should do validation.\n",
    "def calculate_error_normalization_by_JPEG_quality(config, calibration_depth_map, calibration_mask, image_compression_type, subsampling, low_JPEG_quality, high_JPEG_quality, depth_loss_function, mask_type, mask_loss_function, error_threshold_for_masking):\n",
    "    with torch.no_grad():\n",
    "        # Run as long as JPEG compression is enabled and you are actively training based on depth or mask losses\n",
    "        if not (config.image_compression_type == ImageCompressionType.LOSSLESS or (not config.depth_loss_enabled and not config.mask_loss_enabled)):\n",
    "            recovered_depth, recovered_mask, neural_encoding, y, cb, cr, JPEG_size = net(torch.cat((calibration_depth_map, calibration_mask), 1), low_JPEG_quality, True, image_compression_type)\n",
    "            if config.depth_loss_enabled:\n",
    "                depth_MSE_loss = MSE_criterion(recovered_depth, calibration_depth_map)\n",
    "                depth_L1_loss_low_JPEG_quality = L1_criterion(recovered_depth, calibration_depth_map)\n",
    "                depth_RMSE_loss_low_JPEG_quality = math.sqrt(depth_MSE_loss)\n",
    "                depth_loss_low_JPEG_quality = depth_loss_function(recovered_depth, calibration_depth_map)\n",
    "\n",
    "            if config.mask_loss_enabled:\n",
    "                mask_loss_low_JPEG_quality = None\n",
    "                if mask_type == MaskType.BINARY:\n",
    "                    mask_loss_low_JPEG_quality = mask_loss_function(recovered_mask, calibration_mask)\n",
    "                elif mask_type == MaskType.ERROR_THRESHOLDED_BINARY:\n",
    "                    mask_loss_low_JPEG_quality = mask_loss_function(recovered_mask, calibration_mask * (torch.abs(calibration_depth_map - recovered_depth * calibration_mask) < error_threshold_for_masking))\n",
    "                elif mask_type == MaskType.ERROR_MAP:\n",
    "                    # mask_loss_low_JPEG_quality = mask_loss_function(recovered_mask, torch.abs(calibration_depth_map - recovered_depth) * calibration_mask)\n",
    "                    mask_loss_low_JPEG_quality = mask_loss_function(recovered_mask,  torch.nn.Tanh()(torch.abs(calibration_depth_map - recovered_depth) * 30) * calibration_mask + (1 - calibration_mask))\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid image compression type selected!\")\n",
    "                \n",
    "            recovered_depth, recovered_mask, neural_encoding, y, cb, cr, JPEG_size = net(torch.cat((calibration_depth_map, calibration_mask), 1), high_JPEG_quality, True, image_compression_type)\n",
    "            \n",
    "            if config.depth_loss_enabled:\n",
    "                depth_MSE_loss = MSE_criterion(recovered_depth, calibration_depth_map)\n",
    "                depth_L1_loss_high_JPEG_quality = L1_criterion(recovered_depth, calibration_depth_map)\n",
    "                depth_RMSE_loss_high_JPEG_quality = math.sqrt(depth_MSE_loss)\n",
    "                depth_loss_high_JPEG_quality = depth_loss_function(recovered_depth, calibration_depth_map)\n",
    "\n",
    "            if config.mask_loss_enabled:\n",
    "                mask_loss_high_JPEG_quality = None\n",
    "                if mask_type == MaskType.BINARY:\n",
    "                    mask_loss_high_JPEG_quality = mask_loss_function(recovered_mask, calibration_mask)\n",
    "                elif mask_type == MaskType.ERROR_THRESHOLDED_BINARY:\n",
    "                    mask_loss_high_JPEG_quality = mask_loss_function(recovered_mask, calibration_mask * (torch.abs(calibration_depth_map - recovered_depth * calibration_mask) < error_threshold_for_masking))\n",
    "                elif mask_type == MaskType.ERROR_MAP:\n",
    "                    # mask_loss_high_JPEG_quality = mask_loss_function(recovered_mask, torch.abs(calibration_depth_map - recovered_depth) * calibration_mask)\n",
    "                    mask_loss_high_JPEG_quality = mask_loss_function(recovered_mask,  torch.nn.Tanh()(torch.abs(calibration_depth_map - recovered_depth) * 30) * calibration_mask + (1 - calibration_mask))\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid image compression type selected!\")\n",
    "                    \n",
    "            if config.depth_loss_enabled:\n",
    "                depth_RMSE_JPEG_quality_normalization_factor = (depth_RMSE_loss_low_JPEG_quality - depth_RMSE_loss_high_JPEG_quality) / depth_RMSE_loss_high_JPEG_quality\n",
    "                depth_L1_JPEG_quality_normalization_factor = (depth_L1_loss_low_JPEG_quality - depth_L1_loss_high_JPEG_quality) / depth_L1_loss_high_JPEG_quality\n",
    "                depth_loss_JPEG_quality_normalization_factor = (depth_loss_low_JPEG_quality - depth_loss_high_JPEG_quality) / depth_loss_high_JPEG_quality\n",
    "\n",
    "                avg_depth_L1_loss = (depth_L1_loss_low_JPEG_quality + depth_L1_loss_high_JPEG_quality) / 2\n",
    "                avg_depth_RMSE_loss = (depth_RMSE_loss_low_JPEG_quality + depth_RMSE_loss_high_JPEG_quality) / 2\n",
    "            else:\n",
    "                depth_loss_JPEG_quality_normalization_factor, depth_RMSE_JPEG_quality_normalization_factor, depth_L1_JPEG_quality_normalization_factor, avg_depth_L1_loss, avg_depth_RMSE_loss = 1, 1, 1, torch.nan, torch.nan\n",
    "                \n",
    "            if config.mask_loss_enabled:\n",
    "                mask_JPEG_quality_normalization_factor = (mask_loss_low_JPEG_quality - mask_loss_high_JPEG_quality) / mask_loss_high_JPEG_quality\n",
    "\n",
    "                avg_mask_loss = (mask_loss_low_JPEG_quality + mask_loss_high_JPEG_quality) / 2\n",
    "            else:\n",
    "                mask_JPEG_quality_normalization_factor, avg_mask_loss = torch.nan, torch.nan\n",
    "\n",
    "        else:\n",
    "            depth_RMSE_JPEG_quality_normalization_factor = 1\n",
    "            depth_L1_JPEG_quality_normalization_factor = 1\n",
    "            depth_loss_JPEG_quality_normalization_factor = 1\n",
    "            mask_JPEG_quality_normalization_factor = 1\n",
    "            avg_depth_L1_loss = torch.nan\n",
    "            avg_depth_RMSE_loss = torch.nan\n",
    "            avg_mask_loss = torch.nan\n",
    "\n",
    "    return depth_RMSE_JPEG_quality_normalization_factor, depth_L1_JPEG_quality_normalization_factor, depth_loss_JPEG_quality_normalization_factor, mask_JPEG_quality_normalization_factor, avg_depth_L1_loss, avg_depth_RMSE_loss, avg_mask_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.00979,
     "end_time": "2024-10-07T10:47:48.142670",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.132880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I was really focused on ~equal performance across the whole normalized depth range, so this was effectively my \"validation loss\".\n",
    "# Note that I'm not using differentiable rounding since there is no need for backpropagation.\n",
    "def validate_depth_gradient_image_performance(depth_gradient_image, depth_gradient_image_and_ones_mask, depth_gradient_image_and_zeros_mask, target_validation_JPEG_quality, subsampling, gradient_validation_image_compression_type, save_RGB_encodings=False):\n",
    "    with torch.no_grad():\n",
    "        if config.depth_loss_enabled or config.mask_loss_enabled:\n",
    "            # Note: neural_encoding_for_ones_mask, the RGB encodings corresponding to 10,000 depth values in the range [0,1] can be saved to images and later plotted in 3D for analysis.\n",
    "            decoded_depth, _, neural_encoding_for_ones_mask, _, _, _, _ = net(depth_gradient_image_and_ones_mask, target_validation_JPEG_quality, False, gradient_validation_image_compression_type)\n",
    "            \n",
    "            depth_range_gradient_L1_loss = nn.L1Loss()(depth_gradient_image, decoded_depth)\n",
    "            max_depth_range_gradient_L1_loss = torch.max(torch.abs(depth_gradient_image - decoded_depth))\n",
    "        else:\n",
    "            depth_range_gradient_L1_loss = torch.nan\n",
    "            max_depth_range_gradient_L1_loss = torch.nan\n",
    "\n",
    "        if save_RGB_encodings:\n",
    "            # Note: neural_encoding_for_zeros_mask should converge to being a mostly uniform color since likely a single color be selected to denote \"background\" pixels.\n",
    "            _, _, neural_encoding_for_zeros_mask, _, _, _, _ = net(depth_gradient_image_and_zeros_mask, target_validation_JPEG_quality, False, gradient_validation_image_compression_type, True, False)\n",
    "        else:\n",
    "            neural_encoding_for_zeros_mask = None\n",
    "\n",
    "        return depth_range_gradient_L1_loss, max_depth_range_gradient_L1_loss, neural_encoding_for_ones_mask, neural_encoding_for_zeros_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.008906,
     "end_time": "2024-10-07T10:47:48.157548",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.148642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful if you want to check on the current learning rate (due to use of learning rate schedulers)\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.011775,
     "end_time": "2024-10-07T10:47:48.175435",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.163660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the main model inference function, which is called from the training loop.\n",
    "def inference_and_loss_calculation(config, depth_maps, ground_truth_mask, quality, depth_RMSE_JPEG_quality_normalization_factor, depth_L1_JPEG_quality_normalization_factor, depth_loss_JPEG_quality_normalization_factor, mask_JPEG_quality_normalization_factor):\n",
    "    # Inference\n",
    "    recovered_depth, recovered_mask, _, _, _, _, _ = net(torch.cat((depth_maps, ground_truth_mask), 1), quality, True, config.image_compression_type)\n",
    "\n",
    "    combined_loss = 0\n",
    "    \n",
    "    # Depth Loss\n",
    "    if config.depth_loss_enabled:\n",
    "        depth_loss = config.depth_loss_function(recovered_depth * ground_truth_mask, depth_maps * ground_truth_mask)\n",
    "        depth_MSE_loss = MSE_criterion(recovered_depth * ground_truth_mask, depth_maps * ground_truth_mask)\n",
    "        depth_L1_loss = L1_criterion(recovered_depth * ground_truth_mask, depth_maps * ground_truth_mask)\n",
    "        depth_RMSE_loss = torch.sqrt(depth_MSE_loss)\n",
    "\n",
    "        # Normalization of losses across JPEG qualities\n",
    "        reweighted_depth_loss = depth_loss * (1 + depth_loss_JPEG_quality_normalization_factor * (quality - config.low_JPEG_quality) / (config.high_JPEG_quality - config.low_JPEG_quality))\n",
    "        reweighted_depth_RMSE_loss = depth_RMSE_loss * (1 + depth_RMSE_JPEG_quality_normalization_factor * (quality - config.low_JPEG_quality) / (config.high_JPEG_quality - config.low_JPEG_quality))  # Just for printing losses (not used for backpropagation)\n",
    "        reweighted_depth_L1_loss = depth_L1_loss * (1 + depth_L1_JPEG_quality_normalization_factor * (quality - config.low_JPEG_quality) / (config.high_JPEG_quality - config.low_JPEG_quality))  # Just for printing losses (not used for backpropagation)\n",
    "\n",
    "        # Add reweighted mask loss to combined loss (used for backpropagation)\n",
    "        combined_loss += config.depth_loss_weighting * reweighted_depth_loss\n",
    "    else:\n",
    "        reweighted_depth_loss = torch.nan\n",
    "        reweighted_depth_RMSE_loss = torch.nan\n",
    "        reweighted_depth_L1_loss = torch.nan\n",
    "        \n",
    "\n",
    "    # Mask Loss\n",
    "    mask_loss = torch.nan\n",
    "    if config.mask_loss_enabled:\n",
    "        # print(\"using mask loss\")\n",
    "        if config.mask_type == MaskType.BINARY:\n",
    "            mask_loss = config.mask_loss_function(recovered_mask, ground_truth_mask.float())\n",
    "        elif config.mask_type == MaskType.ERROR_THRESHOLDED_BINARY:\n",
    "            # Remove depth entries with recovered depth errors above the specified error threshold from the ground truth mask\n",
    "            mask_loss = config.mask_loss_function(recovered_mask, (ground_truth_mask.float() * (torch.abs(depth_maps * ground_truth_mask - recovered_depth * ground_truth_mask) < config.error_threshold_for_masking)).detach())\n",
    "        elif config.mask_type == MaskType.ERROR_MAP:\n",
    "            mask_loss = config.mask_loss_function(recovered_mask, torch.nn.Tanh()(torch.abs(depth_maps * ground_truth_mask - recovered_depth * ground_truth_mask) * 30) + (1 - ground_truth_mask.float()))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mask type type selected!\")\n",
    "\n",
    "        # Normalization of losses across JPEG qualities\n",
    "        reweighted_mask_loss = mask_loss * (1 + mask_JPEG_quality_normalization_factor * (quality - config.low_JPEG_quality) / (config.high_JPEG_quality - config.low_JPEG_quality))\n",
    "\n",
    "        # Add reweighted mask loss to combined loss (used for backpropagation)\n",
    "        combined_loss += config.mask_loss_weighting * reweighted_mask_loss\n",
    "    else:\n",
    "        reweighted_mask_loss = torch.nan\n",
    "\n",
    "    return combined_loss, mask_loss, reweighted_depth_RMSE_loss, reweighted_depth_L1_loss, reweighted_mask_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009215,
     "end_time": "2024-10-07T10:47:48.190767",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.181552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I wanted equal performance across a normalized depth range, so this image histogram equalization was important to allow me to equally sample across [0,1]\n",
    "def image_histogram_equalization(image, number_bins=1024):\n",
    "    # From http://www.janeriksolem.net/histogram-equalization-with-python-and.html\n",
    "\n",
    "    # Get image histogram\n",
    "    image_histogram, bins = np.histogram(image.flatten(), number_bins, density=True)\n",
    "    cdf = (image_histogram*1025).cumsum() # cumulative distribution function\n",
    "    cdf = (number_bins-1) * cdf / cdf[-1] # normalize\n",
    "\n",
    "    # Use linear interpolation of cdf to find new pixel values\n",
    "    image_equalized = np.interp(image.flatten(), bins[:-1], cdf)\n",
    "\n",
    "    return image_equalized.reshape(image.shape), cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009272,
     "end_time": "2024-10-07T10:47:48.206139",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.196867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_optimizer(config, net):    \n",
    "    if config.optimizer_type == optim.Adam:\n",
    "        optimizer = optim.Adam(net.parameters(), lr=config.starting_lr, weight_decay=config.weight_decay)\n",
    "    elif config.optimizer_type == optim.SGD:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=config.starting_lr, momentum=0.9, weight_decay=config.weight_decay, nesterov=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer type selected!\")\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009363,
     "end_time": "2024-10-07T10:47:48.221524",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.212161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_scheduler(config, optimizer):\n",
    "    if config.scheduler_type == optim.lr_scheduler.CosineAnnealingWarmRestarts:\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, config.cosine_annealing_scheduler_period, config.cosine_annealing_scheduler_restart_factor)\n",
    "    elif config.scheduler_type == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=config.lr_gamma, patience=config.patience, threshold=0.0001, threshold_mode='rel', cooldown=config.cooldown, min_lr=0, eps=1e-10)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scheduler type selected!\")\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.00606,
     "end_time": "2024-10-07T10:47:48.233558",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.227498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare Save Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.010093,
     "end_time": "2024-10-07T10:47:48.249710",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.239617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting a naming format for checkpoint files and preparing the folder structures\n",
    "notebook_name = os.path.splitext(notebook_name)[0]\n",
    "save_directory_all_runs = \"./weights/\" + notebook_name\n",
    "training_run_number, weight_save_directory, config, checkpoint_file_path = determine_training_run_number(config, save_directory_all_runs)\n",
    "create_images_folder_structure(config, notebook_name)\n",
    "print(f'weight_save_directory = {weight_save_directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21162ed-ccad-4cd0-bc08-4a26432b5c96",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.009338,
     "end_time": "2024-10-07T10:47:48.265103",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.255765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(config.start_from_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.007355,
     "end_time": "2024-10-07T10:47:48.279294",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.271939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.013924,
     "end_time": "2024-10-07T10:47:48.302196",
     "exception": false,
     "start_time": "2024-10-07T10:47:48.288272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorBoard Logging\n",
    "if not (config.dry_run or config.debug_mode):\n",
    "    writer = SummaryWriter(log_dir=\"TensorBoard_logs\", filename_suffix=notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": 0.219192,
     "end_time": "2024-10-07T10:47:48.530829",
     "exception": true,
     "start_time": "2024-10-07T10:47:48.311637",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some data augmentation\n",
    "transforms = T.Compose([\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomVerticalFlip(),\n",
    "                T.Resize((config.height, config.width), interpolation=config.interpolation_mode)\n",
    "                # More augmentation transforms can be added here\n",
    "            ])\n",
    "\n",
    "# Initialize training dataset\n",
    "dataset = FlyingThingsDataset(config.dataset_path, \"TRAIN\", config.random_crops, transforms)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=1, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Neural Network Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = NeuralDepthAutoencoder(config, config.height, config.width, config.subsampling, config.color_space_conversions, config.device).to(config.device)\n",
    "\n",
    "net.train()  # Important for layers such as batch normalization, if you end up adding them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Optimizer and Scheduler Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = initialize_optimizer(config, net)\n",
    "scheduler = initialize_scheduler(config, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Intantiate various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "starting_epoch = 0  # Epoch number starts with 0, if this is the first training run \n",
    "global_step = 0  # Keeps track of the overall mini-batch number\n",
    "\n",
    "# Initialize these metrics to 0 since they are being minimized\n",
    "best_depth_range_gradient_L1_loss = torch.inf\n",
    "best_max_depth_range_gradient_L1_loss = torch.inf\n",
    "best_avg_L1_loss_on_JPEG_normalization_image = torch.inf\n",
    "best_avg_RMSE_loss_on_JPEG_normalization_image = torch.inf\n",
    "best_avg_mask_loss_on_JPEG_normalization_image = torch.inf\n",
    "\n",
    "# Prepopulate some metric arrays with zeros (list appending or sole use of the TensorBoard logs may be a preferable alternative to you)\n",
    "best_depth_range_gradient_L1_losses_by_epoch = np.zeros(config.num_training_epochs)\n",
    "best_max_depth_range_gradient_L1_losses_by_epoch = np.zeros(config.num_training_epochs)\n",
    "best_avg_L1_losses_on_JPEG_normalization_by_epoch = np.zeros(config.num_training_epochs)\n",
    "best_avg_RMSE_losses_on_JPEG_normalization_by_epoch = np.zeros(config.num_training_epochs)\n",
    "best_avg_mask_loss_on_JPEG_normalization_image_by_epoch = np.zeros(config.num_training_epochs)\n",
    "\n",
    "# May override some of the above parameters if continuing an interrupted training session\n",
    "if config.weight_initialization_source is not None:\n",
    "    load_checkpoint_file(config.weight_initialization_source)  # Load a checkpoint from a different source directory\n",
    "elif not config.start_from_scratch and checkpoint_file_path is not None:\n",
    "    load_checkpoint_file(checkpoint_file_path)  # Load a checkpoint file from an interrupted training session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Instantiate a depth map for normalization of losses across JPEG qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I didn't really use this test dataset for testing.\n",
    "# In this code cell I'm just using a single disparity map to use for error normalization purposes across JPEG quality levels (since I elected not to use a rate-distortion loss).\n",
    "test_dataset = FlyingThingsDataset(config.dataset_path, \"TEST\", False, T.Resize((config.height, config.width), interpolation=config.interpolation_mode))\n",
    "\n",
    "calibration_depth_map = test_dataset[5].reshape([1, 1, 224, 224])\n",
    "\n",
    "calibration_depth_map_normalized, _ = image_histogram_equalization(calibration_depth_map.detach().cpu().numpy(), 1024)\n",
    "calibration_depth_map = torch.from_numpy(calibration_depth_map_normalized).float()\n",
    "\n",
    "minimum = torch.min(calibration_depth_map)\n",
    "maximum = torch.max(calibration_depth_map)\n",
    "val_range = maximum - minimum\n",
    "\n",
    "# Cutoff values so that there is some \"background\" region to mask out\n",
    "low_cutoff = minimum + 0.1*val_range\n",
    "high_cutoff = maximum - 0.1*val_range\n",
    "\n",
    "calibration_mask1 = calibration_depth_map > low_cutoff\n",
    "calibration_mask2 = calibration_depth_map < high_cutoff\n",
    "calibration_mask = calibration_mask1 * calibration_mask2\n",
    "calibration_depth_map = calibration_depth_map * calibration_mask\n",
    "\n",
    "calibration_depth_map = (calibration_depth_map.view(config.batch_size, 1, config.height, config.width) - low_cutoff) / (high_cutoff - low_cutoff)\n",
    "calibration_depth_map = calibration_depth_map * calibration_mask\n",
    "calibration_depth_map = calibration_depth_map.to(config.device)\n",
    "\n",
    "calibration_depth_map = torch.clone(calibration_depth_map)\n",
    "calibration_mask = torch.clone(calibration_mask).float().to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization of the depth map used for normalizing the losses across different JPEG qualities\n",
    "plt.imshow(calibration_depth_map.cpu().numpy().reshape([224,224]))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Instantiate a depth gradient image (i.e., tilted plane) for lossless validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 100x100 Depth range gradient image to serve as a sort of validation image\n",
    "depth_gradient_image = torch.zeros(10000).to(config.device)\n",
    "for i in range(10000):\n",
    "    depth = i/9999\n",
    "    depth_gradient_image[i] = depth\n",
    "depth_gradient_image = depth_gradient_image.reshape([1, 1, 100, 100])\n",
    "depth_gradient_image_ones_mask = torch.ones([1, 1, 100, 100]).to(config.device)\n",
    "depth_gradient_image_zeros_mask = torch.zeros([1, 1, 100, 100]).to(config.device)\n",
    "\n",
    "depth_gradient_image_and_ones_mask = torch.cat((depth_gradient_image, depth_gradient_image_ones_mask), 1)  # All \"foreground\"\n",
    "depth_gradient_image_and_zeros_mask = torch.cat((depth_gradient_image, depth_gradient_image_zeros_mask), 1)  # All \"background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1036b9-214f-41d3-896c-d12b757e3fb1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization of the depth gradient image (i.e., a tilted plane)\n",
    "plt.imshow(depth_gradient_image.cpu().numpy().reshape([100,100]))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Instantiate some functions for shorthand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MSE_criterion = nn.MSELoss()\n",
    "L1_criterion = nn.L1Loss()\n",
    "BCE_criterion = nn.BCEWithLogitsLoss()\n",
    "toPILImage = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### For verification of histogram equalization of depth maps (tends to improve training results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_overall_histogram = np.zeros(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training_loop(config, starting_epoch, weight_save_directory, notebook_name, training_run_number, scaler, calibration_depth_map, calibration_mask, depth_gradient_image, depth_gradient_image_and_ones_mask, depth_gradient_image_and_zeros_mask, test_overall_histogram):\n",
    "    if not config.dry_run:\n",
    "        config.ran_to_completion = False\n",
    "        with open(f'{weight_save_directory}{notebook_name}_training_configuration_run{training_run_number}.pt', 'wb') as handle:\n",
    "            pickle.dump(config, handle)\n",
    "\n",
    "    # These were instantiated earlier and they will persist between training runs\n",
    "    global best_depth_range_gradient_L1_loss\n",
    "    global best_max_depth_range_gradient_L1_loss\n",
    "    global best_avg_L1_loss_on_JPEG_normalization_image\n",
    "    global best_avg_RMSE_loss_on_JPEG_normalization_image\n",
    "    global best_avg_mask_loss_on_JPEG_normalization_image\n",
    "    global best_depth_range_gradient_L1_losses_by_epoch\n",
    "    global best_max_depth_range_gradient_L1_losses_by_epoch\n",
    "    global best_avg_L1_losses_on_JPEG_normalization_by_epoch\n",
    "    global best_avg_RMSE_losses_on_JPEG_normalization_by_epoch\n",
    "    global best_avg_mask_loss_on_JPEG_normalization_image_by_epoch\n",
    "    global global_step\n",
    "\n",
    "    last_epoch_saved = -1  # Used to tell if a model checkpoint has occurred for the current epoch\n",
    "    \n",
    "    for epoch in range(starting_epoch, config.num_training_epochs):\n",
    "        progress_bar = tqdm(train_loader, miniters=config.print_interval, maxinterval=1e9)\n",
    "        num_samples = len(progress_bar)\n",
    "\n",
    "        # For performance reasons, training checkpoints are cached into a dictionary and saved to disk at the end of every epoch\n",
    "        cached_checkpoints = {}\n",
    "\n",
    "        # Loop over all training samples (1 epoch)\n",
    "        for i, depth_maps in enumerate(progress_bar):\n",
    "            # Randomly select a JPEG quality level from the specified range\n",
    "            quality = random.uniform(config.low_JPEG_quality, config.high_JPEG_quality)\n",
    "\n",
    "            # Histogram equalization of depth map to improve depth range sampling pattern\n",
    "            depth_maps_histogram_normalized, _ = image_histogram_equalization(depth_maps.detach().cpu().numpy(), 1024)\n",
    "            depth_maps = torch.from_numpy(depth_maps_histogram_normalized).float()\n",
    "\n",
    "            minimum = torch.min(depth_maps)\n",
    "            maximum = torch.max(depth_maps)\n",
    "            val_range = maximum - minimum\n",
    "\n",
    "            # Randomly cut off a portion of the depth range to create \"background\" regions for the ground truth mask\n",
    "            low_cutoff = minimum + random.uniform(0.0, 0.3)*val_range\n",
    "            high_cutoff = maximum - random.uniform(0.0, 0.3)*val_range\n",
    "\n",
    "            # Just in case\n",
    "            if high_cutoff <= low_cutoff:\n",
    "                raise Exception(\"The depth range low and high cutoff values are invalid\")\n",
    "\n",
    "            # Creating a ground truth mask and applying it to the ground truth depth map(s)\n",
    "            mask1 = depth_maps > low_cutoff\n",
    "            mask2 = depth_maps < high_cutoff\n",
    "            ground_truth_mask = mask1 * mask2\n",
    "            depth_maps = depth_maps * ground_truth_mask\n",
    "\n",
    "            # Extended normalized depth range by +1%/-1% (configurable) during training to achieve accurate results near extrema during inference\n",
    "            depth_maps = ((depth_maps.view(config.batch_size, 1, config.height, config.width) - low_cutoff) / (high_cutoff - low_cutoff)* config.expanded_training_range - (config.expanded_training_range - 1) / 2) * (config.depth_normalization_upper_bound - config.depth_normalization_lower_bound) + config.depth_normalization_lower_bound\n",
    "            depth_maps = depth_maps * ground_truth_mask\n",
    "\n",
    "            # Zero gradients so they don't accumulate\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            depth_maps = depth_maps.to(config.device)\n",
    "            ground_truth_mask = ground_truth_mask.to(config.device)\n",
    "\n",
    "            # Periodically calculate loss normalization factors since I didn't use a rate-distortion loss and am randomizing the JPEG quality\n",
    "            if i % config.JPEG_normalization_interval == 0:\n",
    "                depth_RMSE_JPEG_quality_normalization_factor, depth_L1_JPEG_quality_normalization_factor, depth_loss_JPEG_quality_normalization_factor, mask_JPEG_quality_normalization_factor, avg_L1_loss_on_JPEG_normalization_image, avg_RMSE_loss_on_JPEG_normalization_image, avg_mask_loss_on_JPEG_normalization_image = calculate_error_normalization_by_JPEG_quality(config, calibration_depth_map, calibration_mask, config.image_compression_type, config.subsampling, config.low_JPEG_quality, config.high_JPEG_quality, config.depth_loss_function, config.mask_type, config.mask_loss_function, config.error_threshold_for_masking)\n",
    "\n",
    "            # Bool to control periodic image saving (plot the values from the resulting images in 3D to see the encoding pattern in the RGB color space)\n",
    "            log_encoded_images = (global_step % config.encoded_image_log_interval == 0 and not (config.dry_run or config.debug_mode))\n",
    "                \n",
    "            # Depth gradient \"validation\" losses on a depth gradient image that samples full [0,1] depth range (i.e., a \"tilted plane\")\n",
    "            if i % config.depth_gradient_val_interval == 0 or global_step % config.encoded_image_log_interval == 0:\n",
    "                depth_range_gradient_L1_loss, max_depth_range_gradient_L1_loss, neural_encoding_for_ones_mask, neural_encoding_for_zeros_mask = validate_depth_gradient_image_performance(depth_gradient_image, depth_gradient_image_and_ones_mask, depth_gradient_image_and_zeros_mask, config.target_validation_JPEG_quality, config.subsampling, config.gradient_validation_image_compression_type, log_encoded_images)                \n",
    "            \n",
    "            # Log lossless encoding images\n",
    "            if not config.dry_run:\n",
    "                if log_encoded_images:\n",
    "                    # This will log the encoded colors for the depth range [0, 1.0] for 10,000 entries.  Plot these in 3D for best visualization.\n",
    "                    image_save_name = \"./images/\" + notebook_name + \"/neural_encoding_for_ones_mask/\" + notebook_name + \"_neural_encoding_for_ones_mask_%08d.png\" % global_step \n",
    "                    toPILImage(neural_encoding_for_ones_mask.squeeze()).save(image_save_name, \"PNG\")\n",
    "\n",
    "                    # This should result in \"background\" encoding color (likely just a single color, more or less).\n",
    "                    image_save_name = \"./images/\" + notebook_name + \"/neural_encoding_for_zeros_mask/\" + notebook_name + \"_neural_encoding_for_zeros_mask_%08d.png\" % global_step \n",
    "                    toPILImage(neural_encoding_for_zeros_mask.squeeze()).save(image_save_name, \"PNG\")\n",
    "\n",
    "            # Inference and loss calculations\n",
    "            if config.precision_mode == PrecisionMode.MIXED_PRECISION:\n",
    "                with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    combined_loss, mask_loss, reweighted_depth_RMSE_loss, reweighted_depth_L1_loss, reweighted_mask_loss = inference_and_loss_calculation(config, depth_maps, ground_truth_mask, quality, depth_RMSE_JPEG_quality_normalization_factor, depth_L1_JPEG_quality_normalization_factor, depth_loss_JPEG_quality_normalization_factor, mask_JPEG_quality_normalization_factor)\n",
    "            else:\n",
    "                combined_loss, mask_loss, reweighted_depth_RMSE_loss, reweighted_depth_L1_loss, reweighted_mask_loss = inference_and_loss_calculation(config, depth_maps, ground_truth_mask, quality, depth_RMSE_JPEG_quality_normalization_factor, depth_L1_JPEG_quality_normalization_factor, depth_loss_JPEG_quality_normalization_factor, mask_JPEG_quality_normalization_factor)\n",
    "                \n",
    "            # Saving best losses and checkpointing\n",
    "            if depth_range_gradient_L1_loss < best_depth_range_gradient_L1_loss:\n",
    "                best_depth_range_gradient_L1_loss = depth_range_gradient_L1_loss\n",
    "\n",
    "                # This is my favorite loss to checkpoint on (I largely do not use the others, but they can be useful if you encounter overfitting)\n",
    "                cached_checkpoints[\"best_L1_rangeGradient_val_loss\"] = cache_checkpoint(config, epoch, net, optimizer, scheduler, scaler, \n",
    "                    best_depth_range_gradient_L1_loss, best_max_depth_range_gradient_L1_loss, \n",
    "                    best_avg_L1_loss_on_JPEG_normalization_image, best_avg_RMSE_loss_on_JPEG_normalization_image, best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                    best_depth_range_gradient_L1_losses_by_epoch, best_max_depth_range_gradient_L1_losses_by_epoch, best_avg_L1_losses_on_JPEG_normalization_by_epoch, \n",
    "                    best_avg_RMSE_losses_on_JPEG_normalization_by_epoch, best_avg_mask_loss_on_JPEG_normalization_image_by_epoch)\n",
    "                \n",
    "            if max_depth_range_gradient_L1_loss < best_max_depth_range_gradient_L1_loss:\n",
    "                best_max_depth_range_gradient_L1_loss = max_depth_range_gradient_L1_loss\n",
    "\n",
    "                cached_checkpoints[\"best_max_rangeGradient_val_loss\"] = cache_checkpoint(config, epoch, net, optimizer, scheduler, scaler, \n",
    "                    best_depth_range_gradient_L1_loss, best_max_depth_range_gradient_L1_loss, \n",
    "                    best_avg_L1_loss_on_JPEG_normalization_image, best_avg_RMSE_loss_on_JPEG_normalization_image, best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                    best_depth_range_gradient_L1_losses_by_epoch, best_max_depth_range_gradient_L1_losses_by_epoch, best_avg_L1_losses_on_JPEG_normalization_by_epoch, \n",
    "                    best_avg_RMSE_losses_on_JPEG_normalization_by_epoch, best_avg_mask_loss_on_JPEG_normalization_image_by_epoch)\n",
    "                \n",
    "            if avg_L1_loss_on_JPEG_normalization_image < best_avg_L1_loss_on_JPEG_normalization_image:\n",
    "                best_avg_L1_loss_on_JPEG_normalization_image = avg_L1_loss_on_JPEG_normalization_image\n",
    "\n",
    "                cached_checkpoints[\"best_avg_L1_val_loss_on_JPEG_normalization_image\"] = cache_checkpoint(config, epoch, net, optimizer, scheduler, scaler, \n",
    "                    best_depth_range_gradient_L1_loss, best_max_depth_range_gradient_L1_loss, \n",
    "                    best_avg_L1_loss_on_JPEG_normalization_image, best_avg_RMSE_loss_on_JPEG_normalization_image, best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                    best_depth_range_gradient_L1_losses_by_epoch, best_max_depth_range_gradient_L1_losses_by_epoch, best_avg_L1_losses_on_JPEG_normalization_by_epoch, \n",
    "                    best_avg_RMSE_losses_on_JPEG_normalization_by_epoch, best_avg_mask_loss_on_JPEG_normalization_image_by_epoch)\n",
    "                \n",
    "            if avg_RMSE_loss_on_JPEG_normalization_image  < best_avg_RMSE_loss_on_JPEG_normalization_image:\n",
    "                best_avg_RMSE_loss_on_JPEG_normalization_image = avg_RMSE_loss_on_JPEG_normalization_image \n",
    "\n",
    "                cached_checkpoints[\"best_avg_RMSE_loss_on_JPEG_normalization_image\"] = cache_checkpoint(config, epoch, net, optimizer, scheduler, scaler, \n",
    "                    best_depth_range_gradient_L1_loss, best_max_depth_range_gradient_L1_loss, \n",
    "                    best_avg_L1_loss_on_JPEG_normalization_image, best_avg_RMSE_loss_on_JPEG_normalization_image, best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                    best_depth_range_gradient_L1_losses_by_epoch, best_max_depth_range_gradient_L1_losses_by_epoch, best_avg_L1_losses_on_JPEG_normalization_by_epoch, \n",
    "                    best_avg_RMSE_losses_on_JPEG_normalization_by_epoch, best_avg_mask_loss_on_JPEG_normalization_image_by_epoch)\n",
    "                \n",
    "            if avg_mask_loss_on_JPEG_normalization_image < best_avg_mask_loss_on_JPEG_normalization_image:\n",
    "                best_avg_mask_loss_on_JPEG_normalization_image = avg_mask_loss_on_JPEG_normalization_image\n",
    "\n",
    "                cached_checkpoints[\"best_avg_mask_loss_on_JPEG_normalization_image\"] = cache_checkpoint(config, epoch, net, optimizer, scheduler, scaler, \n",
    "                    best_depth_range_gradient_L1_loss, best_max_depth_range_gradient_L1_loss, \n",
    "                    best_avg_L1_loss_on_JPEG_normalization_image, best_avg_RMSE_loss_on_JPEG_normalization_image, best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                    best_depth_range_gradient_L1_losses_by_epoch, best_max_depth_range_gradient_L1_losses_by_epoch, best_avg_L1_losses_on_JPEG_normalization_by_epoch, \n",
    "                    best_avg_RMSE_losses_on_JPEG_normalization_by_epoch, best_avg_mask_loss_on_JPEG_normalization_image_by_epoch)\n",
    "           \n",
    "            # Also save once per epoch, regardless of performance on any metrics\n",
    "            if (scheduler.get_last_lr()[0] == config.starting_lr and last_epoch_saved < epoch) or (config.debug_mode and i == config.debug_mode_loop_length - 1):\n",
    "                last_epoch_saved = epoch\n",
    "                \n",
    "                cached_checkpoints[\"epoch%i\" % epoch] = cache_checkpoint(config, epoch, net, optimizer, scheduler, scaler, \n",
    "                    best_depth_range_gradient_L1_loss, best_max_depth_range_gradient_L1_loss, \n",
    "                    best_avg_L1_loss_on_JPEG_normalization_image, best_avg_RMSE_loss_on_JPEG_normalization_image, best_avg_mask_loss_on_JPEG_normalization_image,\n",
    "                    best_depth_range_gradient_L1_losses_by_epoch, best_max_depth_range_gradient_L1_losses_by_epoch, best_avg_L1_losses_on_JPEG_normalization_by_epoch, \n",
    "                    best_avg_RMSE_losses_on_JPEG_normalization_by_epoch, best_avg_mask_loss_on_JPEG_normalization_image_by_epoch)\n",
    "\n",
    "            # Backpropagation and model updates\n",
    "            if config.precision_mode == PrecisionMode.MIXED_PRECISION:\n",
    "                scaler.scale(combined_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                combined_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Learning rate scheduler step\n",
    "            if config.scheduler_type is optim.lr_scheduler.ReduceLROnPlateau:\n",
    "                scheduler.step(best_depth_range_gradient_L1_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Checking that histogram equalization worked\n",
    "            histogram, bin_edges = np.histogram(depth_maps.reshape([config.height, config.width]).detach().cpu().numpy(), bins=1024, range=(-0.01, 1.01))\n",
    "            test_overall_histogram += histogram\n",
    "\n",
    "            # Update progress bar\n",
    "            if global_step % config.print_interval == 0 or i == (num_samples - 1):  # Rate limiting TQDM updates to prevent Jupyter Notebook from complaining\n",
    "                scheduler_lr = scheduler.get_last_lr()[0]\n",
    "                \n",
    "                progress_bar.set_description(f'Run {training_run_number}, Epoch {epoch}, Batch {i}')\n",
    "                progress_metrics = {\n",
    "                    'LR': f'{scheduler_lr:.2E}',\n",
    "                    'Best val': f'{best_depth_range_gradient_L1_loss*1000:.3f}',\n",
    "                    'Best max val': f'{best_max_depth_range_gradient_L1_loss*1000:.3f}',\n",
    "                    'Cal. depth': f'{best_avg_mask_loss_on_JPEG_normalization_image*1000:.3f}',\n",
    "                    'Cal. mask': f'{avg_mask_loss_on_JPEG_normalization_image*1000:.3f}',\n",
    "                    'Val': f'{depth_range_gradient_L1_loss*1000:.3f}',\n",
    "                    'Max val': f'{max_depth_range_gradient_L1_loss*1000:.3f}',\n",
    "                    'RMSE': f'{reweighted_depth_RMSE_loss*1000:.3f}',\n",
    "                    'L1': f'{reweighted_depth_L1_loss*1000:.3f}',\n",
    "                    'Mask loss': f'{reweighted_mask_loss*1000:.3f}',\n",
    "                    'Quality': f'{quality:.3f}'\n",
    "                }\n",
    "                progress_bar.set_postfix(progress_metrics)\n",
    "\n",
    "            # Tensorboard logging\n",
    "            if global_step % config.tensorboard_log_interval == 0 and not (config.dry_run or config.debug_mode):\n",
    "                writer.add_scalar('Training_loss/reweighted_depth_L1_loss', reweighted_depth_L1_loss, global_step)\n",
    "                writer.add_scalar('Training_loss/reweighted_depth_RMSE_loss', reweighted_depth_RMSE_loss, global_step)\n",
    "                writer.add_scalar('Training_loss/reweighted_mask_loss', reweighted_mask_loss, global_step)\n",
    "                \n",
    "                # Most recent validation losses\n",
    "                writer.add_scalar('Validation_loss/depth_range_gradient_L1_loss', depth_range_gradient_L1_loss, global_step)\n",
    "                writer.add_scalar('Validation_loss/max_depth_range_gradient_L1_loss', max_depth_range_gradient_L1_loss, global_step)\n",
    "                writer.add_scalar('Validation_loss/avg_L1_loss_on_JPEG_normalization_image', avg_L1_loss_on_JPEG_normalization_image, global_step)\n",
    "                writer.add_scalar('Validation_loss/avg_RMSE_loss_on_JPEG_normalization_image', avg_RMSE_loss_on_JPEG_normalization_image, global_step)\n",
    "                writer.add_scalar('Validation_loss/avg_mask_loss_on_JPEG_normalization_image', avg_mask_loss_on_JPEG_normalization_image, global_step)\n",
    "                \n",
    "                # Best validation losses\n",
    "                writer.add_scalar('Validation_loss/best_depth_range_gradient_L1_loss', best_depth_range_gradient_L1_loss, global_step)\n",
    "                writer.add_scalar('Validation_loss/best_max_depth_range_gradient_L1_loss', best_max_depth_range_gradient_L1_loss, global_step)\n",
    "                writer.add_scalar('Validation_loss/best_avg_L1_loss_on_JPEG_normalization_image', best_avg_L1_loss_on_JPEG_normalization_image, global_step)\n",
    "                writer.add_scalar('Validation_loss/best_avg_RMSE_loss_on_JPEG_normalization_image', best_avg_RMSE_loss_on_JPEG_normalization_image, global_step)\n",
    "                writer.add_scalar('Validation_loss/best_avg_mask_loss_on_JPEG_normalization_image', best_avg_mask_loss_on_JPEG_normalization_image, global_step)\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Use artificially short epochs if running in debug mode\n",
    "            if config.debug_mode:\n",
    "                if i >= (config.debug_mode_loop_length - 1):\n",
    "                    break\n",
    "\n",
    "        # Save cached checkpoints to disk at the end of the epoch\n",
    "        for checkpoint_save_name, cached_checkpoint in cached_checkpoints.items():\n",
    "            save_checkpoint(cached_checkpoint, checkpoint_save_name, config, weight_save_directory, notebook_name, training_run_number)\n",
    "\n",
    "        # Save best losses to their by-epoch list versions\n",
    "        best_depth_range_gradient_L1_losses_by_epoch[epoch] = best_depth_range_gradient_L1_loss\n",
    "        best_max_depth_range_gradient_L1_losses_by_epoch[epoch] = best_max_depth_range_gradient_L1_loss\n",
    "        best_avg_L1_losses_on_JPEG_normalization_by_epoch[epoch] = best_avg_L1_loss_on_JPEG_normalization_image\n",
    "        best_avg_RMSE_losses_on_JPEG_normalization_by_epoch[epoch] = best_avg_RMSE_loss_on_JPEG_normalization_image\n",
    "        best_avg_mask_loss_on_JPEG_normalization_image_by_epoch[epoch] = best_avg_mask_loss_on_JPEG_normalization_image\n",
    "        \n",
    "        # Flush the TensorBoard outputs after every epoch\n",
    "        if not config.dry_run and not config.debug_mode:\n",
    "            writer.flush()\n",
    "\n",
    "        # End run if progress stalls or we hit the epoch limit for the current run\n",
    "        end_run = False\n",
    "        if config.depth_loss_enabled:\n",
    "            end_run = (epoch - starting_epoch) > (config.stalled_val_patience + 1) and (best_depth_range_gradient_L1_losses_by_epoch[epoch - config.stalled_val_patience] - best_depth_range_gradient_L1_loss) < config.stalled_val_threshold\n",
    "        if config.debug_mode or (epoch - starting_epoch) >= config.max_epochs_per_run:\n",
    "            end_run = True\n",
    "        \n",
    "        if end_run:\n",
    "            config.ran_to_completion = True\n",
    "            with open(f'{weight_save_directory}{notebook_name}_training_configuration_run{training_run_number}.pt', 'wb') as handle:\n",
    "                pickle.dump(config, handle)\n",
    "            break\n",
    "\n",
    "        # Only 1 epoch per run in debug mode\n",
    "        if config.debug_mode:\n",
    "            break\n",
    "        \n",
    "    epoch = epoch + 1\n",
    "    \n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just verification of a couple of important parameters\n",
    "encoder_frozen = not all(param.requires_grad for param in net.encoder.parameters())\n",
    "print(f'encoder_frozen = {encoder_frozen}')\n",
    "encoder_frozen = not all(param.requires_grad for param in net.decoder.parameters())\n",
    "print(f'decoder_frozen = {encoder_frozen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Printing out the network sizes for reference\n",
    "encoder_size = sum(p.numel() for p in net.encoder.parameters())\n",
    "print(f'encoder_size = {encoder_size:,} parameters')\n",
    "decoder_size = sum(p.numel() for p in net.decoder.parameters())\n",
    "print(f'decoder_size = {decoder_size:,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running the training loop the first time (mixed precision mode)\n",
    "if config.precision_mode is PrecisionMode.MIXED_PRECISION:\n",
    "    starting_epoch = run_training_loop(config, starting_epoch, weight_save_directory, notebook_name, training_run_number, scaler, calibration_depth_map, calibration_mask, depth_gradient_image, depth_gradient_image_and_ones_mask, depth_gradient_image_and_zeros_mask, test_overall_histogram)\n",
    "\n",
    "    # Prepare for default PyTorch precision level run\n",
    "    config.start_from_scratch = False\n",
    "    training_run_number, weight_save_directory, config, checkpoint_file_path = determine_training_run_number(config, save_directory_all_runs)\n",
    "    config.precision_mode = PrecisionMode.PYTORCH_DEFAULT\n",
    "    scaler = set_performance_and_precision_settings(config.precision_mode)\n",
    "\n",
    "    # Resetting the optimizer and scheduler for next training run at a new precision level\n",
    "    optimizer = initialize_optimizer(config, net)\n",
    "    scheduler = initialize_scheduler(config, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Default PyTorch Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running the training loop the second time (default PyTorch precision mode)\n",
    "if config.precision_mode is PrecisionMode.PYTORCH_DEFAULT:\n",
    "    starting_epoch = run_training_loop(config, starting_epoch, weight_save_directory, notebook_name, training_run_number, scaler, calibration_depth_map, calibration_mask, depth_gradient_image, depth_gradient_image_and_ones_mask, depth_gradient_image_and_zeros_mask, test_overall_histogram)\n",
    "\n",
    "    # Prepare for the full float32 precision level run\n",
    "    config.start_from_scratch = False\n",
    "    training_run_number, weight_save_directory, config, checkpoint_file_path = determine_training_run_number(config, save_directory_all_runs)\n",
    "    config.precision_mode = PrecisionMode.FLOAT32_PRECISION\n",
    "    scaler = set_performance_and_precision_settings(config.precision_mode)\n",
    "\n",
    "    # Resetting the optimizer and scheduler for next training run at a new precision level\n",
    "    optimizer = initialize_optimizer(config, net)\n",
    "    scheduler = initialize_scheduler(config, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Full float32 Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running the training loop the final time (full float32 precision mode)\n",
    "if config.precision_mode is PrecisionMode.FLOAT32_PRECISION:\n",
    "    starting_epoch = run_training_loop(config, starting_epoch, weight_save_directory, notebook_name, training_run_number, scaler, calibration_depth_map, calibration_mask, depth_gradient_image, depth_gradient_image_and_ones_mask, depth_gradient_image_and_zeros_mask, test_overall_histogram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.929389,
   "end_time": "2024-10-07T10:47:49.152668",
   "environment_variables": {},
   "exception": true,
   "input_path": "N-DEPTH_training_notebook.ipynb",
   "output_path": "N-DEPTH_training_notebook.ipynb",
   "parameters": {
    "PAPERMILL_INPUT_PATH": "N-DEPTH_training_notebook.ipynb"
   },
   "start_time": "2024-10-07T10:47:45.223279",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
